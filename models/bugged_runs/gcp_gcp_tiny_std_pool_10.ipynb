{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKnEMa6zo32N"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/MIDS/chemical_patent_cer_ee/notebooks')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import bert\n",
    "\n",
    "from sre_inputs import *\n",
    "from train_test import *\n",
    "from sre_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzxEMlBo32P"
   },
   "source": [
    "#### BERT Model\n",
    "- Load BERT model and tokenizer\n",
    "- Set max length for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ECV6hEuo32Q"
   },
   "outputs": [],
   "source": [
    "path = '..'\n",
    "\n",
    "# path for bert model\n",
    "bert_model_dir = f'{path}/bert/bert_tiny'\n",
    "bert_type = bert_model_dir.split('/')[-1]\n",
    "\n",
    "# set tokenizer\n",
    "vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# set BERT model\n",
    "bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
    "bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "# set max length for inputs\n",
    "max_length = 512\n",
    "\n",
    "# set parameters for model type\n",
    "marker_type = 'std' # 'em', 'ner', or 'std'\n",
    "head_type = 'pool' # 'cls', 'start', 'pool', or 'ner'\n",
    "subsampled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Data\n",
    "- Upload preprocessed chemical patent file(s)\n",
    "- Use `sre_inputs` module to generate inputs for model\n",
    "- Sample only: split into train/test using `train_test` module\n",
    "- Need to one hot encode labels before using in model\n",
    "\n",
    "*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/sre_em/sre_em_train.csv\n",
      "Loaded ../data/sre_em/sre_em_dev.csv\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV DATASET PROCESSING ####\n",
    "\n",
    "# paths for preprocessed data\n",
    "if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n",
    "elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n",
    "elif marker_type == 'ner' and not subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n",
    "elif marker_type == 'ner' and subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n",
    "\n",
    "print(f'Loaded {train_path}')\n",
    "print(f'Loaded {dev_path}')\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n",
    "    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n",
    "    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_train = [x for x in train_lists[0][:5]]\n",
    "train_labels = train_lists[1]\n",
    "model_labels_train = tf.one_hot(train_labels, depth=3)\n",
    "\n",
    "model_inputs_dev = [x for x in dev_lists[0][:5]]\n",
    "dev_labels = dev_lists[1]\n",
    "model_labels_dev = tf.one_hot(dev_labels, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "by_OKfjhrW0R"
   },
   "outputs": [],
   "source": [
    "#### TEST DATASET PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n",
    "elif marker_type == 'ner':\n",
    "    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_test = [x for x in test_lists[0][:5]]\n",
    "test_labels = test_lists[1]\n",
    "model_labels_test = tf.one_hot(test_labels, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TweKc5oMo32T"
   },
   "source": [
    "#### Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZEgXMIZxrW0S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SRE Max Pool Model ===\n",
      "BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 512, 128), dtype=tf.float32, name=None), name='bert/encoder/layer_1/output/LayerNorm/add_1:0', description=\"created by layer 'bert'\")\n",
      "Prediction: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), name='sre/Softmax:0', description=\"created by layer 'sre'\")\n",
      "\n",
      "Model: \"sre_pool\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e1_mask (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e2_mask (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModelLayer)           (None, 512, 128)     4369408     input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_1 (TFOpLam (None, 512)          0           e1_mask[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_3 (TFOpLam (None, 512)          0           e2_mask[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor (TFOpLambd (None, 512, 128)     0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.count_nonzero (TFOpLamb (None,)              0           tf.convert_to_tensor_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_2 (TFOpLam (None, 512, 128)     0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.count_nonzero_1 (TFOpLa (None,)              0           tf.convert_to_tensor_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.boolean_mask (Slic (None, 128)          0           tf.convert_to_tensor[0][0]       \n",
      "                                                                 tf.convert_to_tensor_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None,)              0           tf.math.count_nonzero[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.boolean_mask_1 (Sl (None, 128)          0           tf.convert_to_tensor_2[0][0]     \n",
      "                                                                 tf.convert_to_tensor_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_1 (TFOpLambda)       (None,)              0           tf.math.count_nonzero_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.RaggedTensor.from_row_length (None, None, 128)    0           tf.compat.v1.boolean_mask[0][0]  \n",
      "                                                                 tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.RaggedTensor.from_row_length (None, None, 128)    0           tf.compat.v1.boolean_mask_1[0][0]\n",
      "                                                                 tf.reshape_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input.to_tensor (InstanceMethod (None, None, 128)    0           tf.RaggedTensor.from_row_lengths[\n",
      "__________________________________________________________________________________________________\n",
      "input.to_tensor_1 (InstanceMeth (None, None, 128)    0           tf.RaggedTensor.from_row_lengths_\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max (TFOpLambda) (None, 128)          0           input.to_tensor[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_1 (TFOpLambd (None, 128)          0           input.to_tensor_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           tf.math.reduce_max[0][0]         \n",
      "                                                                 tf.math.reduce_max_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          65792       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sre (Dense)                     (None, 3)            771         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,435,971\n",
      "Trainable params: 4,435,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1432/1432 [==============================] - 863s 600ms/step - loss: 0.6466 - categorical_accuracy: 0.7258 - recall: 0.6786 - precision: 0.7549 - val_loss: 0.4923 - val_categorical_accuracy: 0.7948 - val_recall: 0.7778 - val_precision: 0.8066\n",
      "Epoch 2/10\n",
      "1432/1432 [==============================] - 853s 596ms/step - loss: 0.4284 - categorical_accuracy: 0.8186 - recall: 0.8036 - precision: 0.8303 - val_loss: 0.3632 - val_categorical_accuracy: 0.8486 - val_recall: 0.8418 - val_precision: 0.8546\n",
      "Epoch 3/10\n",
      "1432/1432 [==============================] - 869s 607ms/step - loss: 0.3412 - categorical_accuracy: 0.8576 - recall: 0.8489 - precision: 0.8644 - val_loss: 0.3231 - val_categorical_accuracy: 0.8693 - val_recall: 0.8611 - val_precision: 0.8752\n",
      "Epoch 4/10\n",
      "1432/1432 [==============================] - 900s 629ms/step - loss: 0.3010 - categorical_accuracy: 0.8767 - recall: 0.8701 - precision: 0.8818 - val_loss: 0.3073 - val_categorical_accuracy: 0.8771 - val_recall: 0.8725 - val_precision: 0.8812\n",
      "Epoch 5/10\n",
      "1432/1432 [==============================] - 879s 614ms/step - loss: 0.2734 - categorical_accuracy: 0.8891 - recall: 0.8840 - precision: 0.8932 - val_loss: 0.2858 - val_categorical_accuracy: 0.8880 - val_recall: 0.8830 - val_precision: 0.8911\n",
      "Epoch 6/10\n",
      "1432/1432 [==============================] - 854s 597ms/step - loss: 0.2513 - categorical_accuracy: 0.8979 - recall: 0.8941 - precision: 0.9012 - val_loss: 0.2866 - val_categorical_accuracy: 0.8907 - val_recall: 0.8885 - val_precision: 0.8927\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432/1432 [==============================] - 855s 597ms/step - loss: 0.2330 - categorical_accuracy: 0.9075 - recall: 0.9042 - precision: 0.9104 - val_loss: 0.2676 - val_categorical_accuracy: 0.8998 - val_recall: 0.8975 - val_precision: 0.9018\n",
      "Epoch 8/10\n",
      "1432/1432 [==============================] - 876s 612ms/step - loss: 0.2157 - categorical_accuracy: 0.9149 - recall: 0.9122 - precision: 0.9177 - val_loss: 0.2562 - val_categorical_accuracy: 0.9043 - val_recall: 0.9022 - val_precision: 0.9063\n",
      "Epoch 9/10\n",
      "1432/1432 [==============================] - 1122s 784ms/step - loss: 0.2019 - categorical_accuracy: 0.9197 - recall: 0.9170 - precision: 0.9222 - val_loss: 0.2524 - val_categorical_accuracy: 0.9076 - val_recall: 0.9052 - val_precision: 0.9094\n",
      "Epoch 10/10\n",
      "1432/1432 [==============================] - 1351s 943ms/step - loss: 0.1868 - categorical_accuracy: 0.9259 - recall: 0.9240 - precision: 0.9277 - val_loss: 0.2340 - val_categorical_accuracy: 0.9148 - val_recall: 0.9123 - val_precision: 0.9169\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV RUN ####\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "train_layers = 0\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model = sre_cls_model(bert, max_length, train_layers)\n",
    "elif head_type == 'start':\n",
    "    model = sre_start_model(bert, max_length, train_layers)\n",
    "elif head_type == 'pool':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "elif head_type == 'ner':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model.fit(\n",
    "        model_inputs_train[:3], \n",
    "        {\"sre\": model_labels_train},\n",
    "        validation_data=(model_inputs_dev[:3], {\"sre\": model_labels_dev}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    model.fit(\n",
    "    model_inputs_train, \n",
    "    {\"sre\": model_labels_train},\n",
    "    validation_data=(model_inputs_dev, {\"sre\": model_labels_dev}),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "578/578 [==============================] - 210s 361ms/step - loss: 0.2279 - categorical_accuracy: 0.9111 - recall: 0.9095 - precision: 0.9128\n",
      "Generate predictions for new samples\n",
      "predictions shape: (18488, 3)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "print('Evaluate on test data')\n",
    "\n",
    "if head_type == 'cls':\n",
    "    results = model.evaluate(model_inputs_test[:3], model_labels_test, batch_size=batch_size)\n",
    "else:\n",
    "    results = model.evaluate(model_inputs_test, model_labels_test, batch_size=batch_size)\n",
    "\n",
    "# generate predictions on new data (probabilities -- the output of the last layer)\n",
    "print(\"Generate predictions for new samples\")\n",
    "\n",
    "if head_type == 'cls':\n",
    "    predictions = model.predict(model_inputs_test[:3])\n",
    "else: \n",
    "    predictions = model.predict(model_inputs_test)\n",
    "\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# save stuff\n",
    "if subsampled:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_sub'\n",
    "else:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}'\n",
    "\n",
    "# save results and predictions\n",
    "outputs = [results, predictions]\n",
    "with open(f'{path}/results/{model_name}.pickle', \"wb\") as f:\n",
    "    pickle.dump(outputs, f)\n",
    "\n",
    "# save model\n",
    "# model.save(f'{path}/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to open saved file\n",
    "# with open(f'{path}/results/{model_name}.pickle', \"rb\") as f:\n",
    "#     saved_outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rV0shhro32V"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: visualize model\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN-Qj8bvCPgA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "colab_sre_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
