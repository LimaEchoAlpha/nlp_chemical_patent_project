{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKnEMa6zo32N"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/MIDS/chemical_patent_cer_ee/notebooks')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import bert\n",
    "\n",
    "from sre_inputs import *\n",
    "from train_test import *\n",
    "from sre_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzxEMlBo32P"
   },
   "source": [
    "#### BERT Model\n",
    "- Load BERT model and tokenizer\n",
    "- Set max length for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ECV6hEuo32Q"
   },
   "outputs": [],
   "source": [
    "path = '..'\n",
    "\n",
    "# path for bert model\n",
    "bert_model_dir = f'{path}/bert/bert_tiny'\n",
    "bert_type = bert_model_dir.split('/')[-1]\n",
    "\n",
    "# set tokenizer\n",
    "vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# set BERT model\n",
    "bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
    "bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "# set max length for inputs\n",
    "max_length = 512\n",
    "\n",
    "# set parameters for model type\n",
    "marker_type = 'std' # 'em', 'ner', or 'std'\n",
    "head_type = 'cls' # 'cls', 'start', 'pool', or 'ner'\n",
    "subsampled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Data\n",
    "- Upload preprocessed chemical patent file(s)\n",
    "- Use `sre_inputs` module to generate inputs for model\n",
    "- Sample only: split into train/test using `train_test` module\n",
    "- Need to one hot encode labels before using in model\n",
    "\n",
    "*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/sre_em/sre_em_train.csv\n",
      "Loaded ../data/sre_em/sre_em_dev.csv\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV DATASET PROCESSING ####\n",
    "\n",
    "# paths for preprocessed data\n",
    "if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n",
    "elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n",
    "elif marker_type == 'ner' and not subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n",
    "elif marker_type == 'ner' and subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n",
    "\n",
    "print(f'Loaded {train_path}')\n",
    "print(f'Loaded {dev_path}')\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n",
    "    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n",
    "    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_train = [x for x in train_lists[0][:5]]\n",
    "train_labels = train_lists[1]\n",
    "model_labels_train = tf.one_hot(train_labels, depth=3)\n",
    "\n",
    "model_inputs_dev = [x for x in dev_lists[0][:5]]\n",
    "dev_labels = dev_lists[1]\n",
    "model_labels_dev = tf.one_hot(dev_labels, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "by_OKfjhrW0R"
   },
   "outputs": [],
   "source": [
    "#### TEST DATASET PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n",
    "elif marker_type == 'ner':\n",
    "    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_test = [x for x in test_lists[0][:5]]\n",
    "test_labels = test_lists[1]\n",
    "model_labels_test = tf.one_hot(test_labels, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TweKc5oMo32T"
   },
   "source": [
    "#### Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZEgXMIZxrW0S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SRE [CLS] Model ===\n",
      "BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 512, 128), dtype=tf.float32, name=None), name='bert/encoder/layer_1/output/LayerNorm/add_1:0', description=\"created by layer 'bert'\")\n",
      "Prediction: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), name='sre/Softmax:0', description=\"created by layer 'sre'\")\n",
      "\n",
      "Model: \"sre_cls\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModelLayer)           (None, 512, 128)     4369408     input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 128)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          33024       tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sre (Dense)                     (None, 3)            771         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,403,203\n",
      "Trainable params: 4,403,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1432/1432 [==============================] - 1342s 933ms/step - loss: 0.7652 - categorical_accuracy: 0.6878 - recall: 0.6355 - precision: 0.7245 - val_loss: 0.6740 - val_categorical_accuracy: 0.7114 - val_recall: 0.5931 - val_precision: 0.8037\n",
      "Epoch 2/10\n",
      "1432/1432 [==============================] - 1328s 927ms/step - loss: 0.6662 - categorical_accuracy: 0.7081 - recall: 0.6200 - precision: 0.7817 - val_loss: 0.6512 - val_categorical_accuracy: 0.7197 - val_recall: 0.6316 - val_precision: 0.7886\n",
      "Epoch 3/10\n",
      "1432/1432 [==============================] - 1305s 911ms/step - loss: 0.6356 - categorical_accuracy: 0.7233 - recall: 0.6362 - precision: 0.7903 - val_loss: 0.6223 - val_categorical_accuracy: 0.7354 - val_recall: 0.6853 - val_precision: 0.7775\n",
      "Epoch 4/10\n",
      "1432/1432 [==============================] - 1292s 903ms/step - loss: 0.6016 - categorical_accuracy: 0.7367 - recall: 0.6741 - precision: 0.7865 - val_loss: 0.5968 - val_categorical_accuracy: 0.7422 - val_recall: 0.6829 - val_precision: 0.7924\n",
      "Epoch 5/10\n",
      "1432/1432 [==============================] - 1274s 890ms/step - loss: 0.5796 - categorical_accuracy: 0.7431 - recall: 0.6829 - precision: 0.7913 - val_loss: 0.5866 - val_categorical_accuracy: 0.7403 - val_recall: 0.6766 - val_precision: 0.7986\n",
      "Epoch 6/10\n",
      "1432/1432 [==============================] - 1285s 897ms/step - loss: 0.5673 - categorical_accuracy: 0.7448 - recall: 0.6861 - precision: 0.7940 - val_loss: 0.6011 - val_categorical_accuracy: 0.7453 - val_recall: 0.6925 - val_precision: 0.7891\n",
      "Epoch 7/10\n",
      "1432/1432 [==============================] - 1279s 893ms/step - loss: 0.5542 - categorical_accuracy: 0.7478 - recall: 0.6868 - precision: 0.7977 - val_loss: 0.6273 - val_categorical_accuracy: 0.7434 - val_recall: 0.7160 - val_precision: 0.7678\n",
      "Epoch 8/10\n",
      "1432/1432 [==============================] - 1293s 903ms/step - loss: 0.5443 - categorical_accuracy: 0.7492 - recall: 0.6866 - precision: 0.7995 - val_loss: 0.5909 - val_categorical_accuracy: 0.7452 - val_recall: 0.6756 - val_precision: 0.8085\n",
      "Epoch 9/10\n",
      "1432/1432 [==============================] - 1284s 897ms/step - loss: 0.5362 - categorical_accuracy: 0.7497 - recall: 0.6888 - precision: 0.8010 - val_loss: 0.6043 - val_categorical_accuracy: 0.7458 - val_recall: 0.6800 - val_precision: 0.8015\n",
      "Epoch 10/10\n",
      "1432/1432 [==============================] - 1266s 884ms/step - loss: 0.5312 - categorical_accuracy: 0.7500 - recall: 0.6856 - precision: 0.8032 - val_loss: 0.5926 - val_categorical_accuracy: 0.7474 - val_recall: 0.6729 - val_precision: 0.8127\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV RUN ####\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "train_layers = 0\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model = sre_cls_model(bert, max_length, train_layers)\n",
    "elif head_type == 'start':\n",
    "    model = sre_start_model(bert, max_length, train_layers)\n",
    "elif head_type == 'pool':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "elif head_type == 'ner':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model.fit(\n",
    "        model_inputs_train[:3], \n",
    "        {\"sre\": model_labels_train},\n",
    "        validation_data=(model_inputs_dev[:3], {\"sre\": model_labels_dev}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    model.fit(\n",
    "    model_inputs_train, \n",
    "    {\"sre\": model_labels_train},\n",
    "    validation_data=(model_inputs_dev, {\"sre\": model_labels_dev}),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "578/578 [==============================] - 159s 273ms/step - loss: 0.5900 - categorical_accuracy: 0.7459 - recall: 0.6740 - precision: 0.8050\n",
      "Generate predictions for new samples\n",
      "predictions shape: (18488, 3)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "print('Evaluate on test data')\n",
    "\n",
    "if head_type == 'cls':\n",
    "    results = model.evaluate(model_inputs_test[:3], model_labels_test, batch_size=batch_size)\n",
    "else:\n",
    "    results = model.evaluate(model_inputs_test, model_labels_test, batch_size=batch_size)\n",
    "\n",
    "# generate predictions on new data (probabilities -- the output of the last layer)\n",
    "print(\"Generate predictions for new samples\")\n",
    "\n",
    "if head_type == 'cls':\n",
    "    predictions = model.predict(model_inputs_test[:3])\n",
    "else: \n",
    "    predictions = model.predict(model_inputs_test)\n",
    "\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# save stuff\n",
    "if subsampled:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_sub'\n",
    "else:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_fixed'\n",
    "\n",
    "# save results and predictions\n",
    "outputs = [results, predictions]\n",
    "with open(f'{path}/results/{model_name}.pickle', \"wb\") as f:\n",
    "    pickle.dump(outputs, f)\n",
    "\n",
    "# save model\n",
    "# model.save(f'{path}/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to open saved file\n",
    "# with open(f'{path}/results/{model_name}.pickle', \"rb\") as f:\n",
    "#     saved_outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3rV0shhro32V"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: visualize model\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN-Qj8bvCPgA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "colab_sre_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
