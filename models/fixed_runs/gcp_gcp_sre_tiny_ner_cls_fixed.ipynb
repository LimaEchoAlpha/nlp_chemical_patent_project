{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKnEMa6zo32N"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/MIDS/chemical_patent_cer_ee/notebooks')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import bert\n",
    "\n",
    "from sre_inputs import *\n",
    "from train_test import *\n",
    "from sre_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzxEMlBo32P"
   },
   "source": [
    "#### BERT Model\n",
    "- Load BERT model and tokenizer\n",
    "- Set max length for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ECV6hEuo32Q"
   },
   "outputs": [],
   "source": [
    "path = '..'\n",
    "\n",
    "# path for bert model\n",
    "bert_model_dir = f'{path}/bert/bert_tiny'\n",
    "bert_type = bert_model_dir.split('/')[-1]\n",
    "\n",
    "# set tokenizer\n",
    "vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# set BERT model\n",
    "bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
    "bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "# set max length for inputs\n",
    "max_length = 512\n",
    "\n",
    "# set parameters for model type\n",
    "marker_type = 'ner' # 'em', 'ner', or 'std'\n",
    "head_type = 'cls' # 'cls', 'start', 'pool', or 'ner'\n",
    "subsampled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Data\n",
    "- Upload preprocessed chemical patent file(s)\n",
    "- Use `sre_inputs` module to generate inputs for model\n",
    "- Sample only: split into train/test using `train_test` module\n",
    "- Need to one hot encode labels before using in model\n",
    "\n",
    "*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/sre_ner/sre_ner_train.csv\n",
      "Loaded ../data/sre_ner/sre_ner_dev.csv\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV DATASET PROCESSING ####\n",
    "\n",
    "# paths for preprocessed data\n",
    "if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n",
    "elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n",
    "elif marker_type == 'ner' and not subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n",
    "elif marker_type == 'ner' and subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n",
    "\n",
    "print(f'Loaded {train_path}')\n",
    "print(f'Loaded {dev_path}')\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n",
    "    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n",
    "    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_train = [x for x in train_lists[0][:5]]\n",
    "train_labels = train_lists[1]\n",
    "model_labels_train = tf.one_hot(train_labels, depth=3)\n",
    "\n",
    "model_inputs_dev = [x for x in dev_lists[0][:5]]\n",
    "dev_labels = dev_lists[1]\n",
    "model_labels_dev = tf.one_hot(dev_labels, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "by_OKfjhrW0R"
   },
   "outputs": [],
   "source": [
    "#### TEST DATASET PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n",
    "elif marker_type == 'ner':\n",
    "    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_test = [x for x in test_lists[0][:5]]\n",
    "test_labels = test_lists[1]\n",
    "model_labels_test = tf.one_hot(test_labels, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TweKc5oMo32T"
   },
   "source": [
    "#### Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZEgXMIZxrW0S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SRE [CLS] Model ===\n",
      "BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 512, 128), dtype=tf.float32, name=None), name='bert/encoder/layer_1/output/LayerNorm/add_1:0', description=\"created by layer 'bert'\")\n",
      "Prediction: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), name='sre/Softmax:0', description=\"created by layer 'sre'\")\n",
      "\n",
      "Model: \"sre_cls\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModelLayer)           (None, 512, 128)     4369408     input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 128)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          33024       tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sre (Dense)                     (None, 3)            771         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,403,203\n",
      "Trainable params: 4,403,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1432/1432 [==============================] - 1495s 1s/step - loss: 0.7453 - categorical_accuracy: 0.6931 - recall: 0.6329 - precision: 0.7350 - val_loss: 0.7141 - val_categorical_accuracy: 0.7039 - val_recall: 0.6486 - val_precision: 0.7532\n",
      "Epoch 2/10\n",
      "1432/1432 [==============================] - 1521s 1s/step - loss: 0.6615 - categorical_accuracy: 0.7130 - recall: 0.6244 - precision: 0.7849 - val_loss: 0.6601 - val_categorical_accuracy: 0.7243 - val_recall: 0.6379 - val_precision: 0.7903\n",
      "Epoch 3/10\n",
      "1432/1432 [==============================] - 1534s 1s/step - loss: 0.6381 - categorical_accuracy: 0.7233 - recall: 0.6294 - precision: 0.7956 - val_loss: 0.6397 - val_categorical_accuracy: 0.7321 - val_recall: 0.6696 - val_precision: 0.7802\n",
      "Epoch 4/10\n",
      "1432/1432 [==============================] - 1516s 1s/step - loss: 0.6123 - categorical_accuracy: 0.7332 - recall: 0.6538 - precision: 0.7962 - val_loss: 0.6069 - val_categorical_accuracy: 0.7421 - val_recall: 0.6895 - val_precision: 0.7904\n",
      "Epoch 5/10\n",
      "1432/1432 [==============================] - 1505s 1s/step - loss: 0.5827 - categorical_accuracy: 0.7476 - recall: 0.6863 - precision: 0.7929 - val_loss: 0.6614 - val_categorical_accuracy: 0.7396 - val_recall: 0.7065 - val_precision: 0.7687\n",
      "Epoch 6/10\n",
      "1432/1432 [==============================] - 1509s 1s/step - loss: 0.5666 - categorical_accuracy: 0.7492 - recall: 0.6960 - precision: 0.7959 - val_loss: 0.6104 - val_categorical_accuracy: 0.7520 - val_recall: 0.7076 - val_precision: 0.7923\n",
      "Epoch 7/10\n",
      "1432/1432 [==============================] - 1568s 1s/step - loss: 0.5528 - categorical_accuracy: 0.7552 - recall: 0.7002 - precision: 0.8007 - val_loss: 0.5951 - val_categorical_accuracy: 0.7523 - val_recall: 0.7050 - val_precision: 0.7951\n",
      "Epoch 8/10\n",
      "1432/1432 [==============================] - 1462s 1s/step - loss: 0.5405 - categorical_accuracy: 0.7577 - recall: 0.7020 - precision: 0.8020 - val_loss: 0.6505 - val_categorical_accuracy: 0.7460 - val_recall: 0.7114 - val_precision: 0.7783\n",
      "Epoch 9/10\n",
      "1432/1432 [==============================] - 1453s 1s/step - loss: 0.5291 - categorical_accuracy: 0.7600 - recall: 0.7045 - precision: 0.8084 - val_loss: 0.5910 - val_categorical_accuracy: 0.7579 - val_recall: 0.6989 - val_precision: 0.8060\n",
      "Epoch 10/10\n",
      "1432/1432 [==============================] - 1457s 1s/step - loss: 0.5211 - categorical_accuracy: 0.7621 - recall: 0.7054 - precision: 0.8087 - val_loss: 0.5758 - val_categorical_accuracy: 0.7571 - val_recall: 0.6954 - val_precision: 0.8074\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV RUN ####\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "train_layers = 0\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model = sre_cls_model(bert, max_length, train_layers)\n",
    "elif head_type == 'start':\n",
    "    model = sre_start_model(bert, max_length, train_layers)\n",
    "elif head_type == 'pool':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "elif head_type == 'ner':\n",
    "    model = sre_pool_model(bert, max_length, train_layers)\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model.fit(\n",
    "        model_inputs_train[:3], \n",
    "        {\"sre\": model_labels_train},\n",
    "        validation_data=(model_inputs_dev[:3], {\"sre\": model_labels_dev}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    model.fit(\n",
    "    model_inputs_train, \n",
    "    {\"sre\": model_labels_train},\n",
    "    validation_data=(model_inputs_dev, {\"sre\": model_labels_dev}),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "578/578 [==============================] - 181s 312ms/step - loss: 0.5866 - categorical_accuracy: 0.7555 - recall: 0.6992 - precision: 0.8020\n",
      "Generate predictions for new samples\n",
      "predictions shape: (18488, 3)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "print('Evaluate on test data')\n",
    "\n",
    "if head_type == 'cls':\n",
    "    results = model.evaluate(model_inputs_test[:3], model_labels_test, batch_size=batch_size)\n",
    "else:\n",
    "    results = model.evaluate(model_inputs_test, model_labels_test, batch_size=batch_size)\n",
    "\n",
    "# generate predictions on new data (probabilities -- the output of the last layer)\n",
    "print(\"Generate predictions for new samples\")\n",
    "\n",
    "if head_type == 'cls':\n",
    "    predictions = model.predict(model_inputs_test[:3])\n",
    "else: \n",
    "    predictions = model.predict(model_inputs_test)\n",
    "\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# save stuff\n",
    "if subsampled:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_sub'\n",
    "else:\n",
    "    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_fixed'\n",
    "\n",
    "# save results and predictions\n",
    "outputs = [results, predictions]\n",
    "with open(f'{path}/results/{model_name}.pickle', \"wb\") as f:\n",
    "    pickle.dump(outputs, f)\n",
    "\n",
    "# save model\n",
    "# model.save(f'{path}/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to open saved file\n",
    "# with open(f'{path}/results/{model_name}.pickle', \"rb\") as f:\n",
    "#     saved_outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3rV0shhro32V"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: visualize model\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN-Qj8bvCPgA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "colab_sre_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
