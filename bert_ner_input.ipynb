{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sentencepiece\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/sample'\n",
    "filename = 'sample_data.csv'\n",
    "full_path = f'{path}/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER labels. \n",
    "    Note that the word can be tokenized to two or more tokens. \n",
    "    Correspondingly, we add - for now - custom 'X' tokens to the labels \n",
    "    in order to maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \n",
    "    *Function modified from BERT_T5_NER_2_3_030521\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    return addDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_ner_inputs(full_path, max_length=400):\n",
    "    \"\"\"\n",
    "    Read the file line by line and construct snippets (\"sentences\"). \n",
    "    A snippet end is marked by '-DOCSTART- -X- O O' in the next row.\n",
    "    \n",
    "    Also, cap snippet length using max_length. \n",
    "    Snippets which are shorter than max_length are padded. \n",
    "    All snippets end with a [SEP] token, padded or not.\n",
    "    \n",
    "    arguments: full path of input file, max length of snippet\n",
    "    returns: lists for BERT inputs (bertSentenceIDs, bertMasks, bertSequenceIDs)\n",
    "             and the inputs before BERT tokenization (sentenceList, nerTokenList, sentLengthList)\n",
    "    \n",
    "    *Function modified from BERT_T5_NER_2_3_030521\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "\n",
    "    # lists for sentences, tokens, labels, etc. \n",
    "    sentenceTokenList = []\n",
    "    nerTokenList = []\n",
    "    sentLengthList = []\n",
    "\n",
    "    # lists for BERT input\n",
    "    bertSentenceIDs = []\n",
    "    bertMasks = []\n",
    "    bertSequenceIDs = []\n",
    "\n",
    "    # always start with [CLS] tokens\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    for line in text:\n",
    "        \n",
    "        parsed_line = line.strip().split('\\t')\n",
    "        \n",
    "        # if new sentence starts\n",
    "        if parsed_line[0][:10] == '-DOCSTART-':\n",
    "            \n",
    "            if len(sentenceTokens) > 1:\n",
    "            \n",
    "                # figure out sentence length for padding or truncating\n",
    "                sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "                sentLengthList.append(sentenceLength)\n",
    "\n",
    "                # Create space for at least a final '[SEP]' token\n",
    "                if sentenceLength >= max_length - 1: \n",
    "                    sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "                    nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "                # add a ['SEP'] token and padding \n",
    "                sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "                nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "                # append current data to lists\n",
    "                sentenceTokenList.append(sentenceTokens)\n",
    "                nerTokenList.append(nerTokens)\n",
    "\n",
    "                # generate BERT tokens\n",
    "                bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "                bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "                bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "                # initialize new sentence\n",
    "                sentenceTokens = ['[CLS]']\n",
    "                nerTokens = ['[nerCLS]']\n",
    "                \n",
    "        elif parsed_line[0]:            \n",
    "            word = parsed_line[0]\n",
    "            ner = parsed_line[1]\n",
    "            \n",
    "            addDict = addWord(word, ner)\n",
    "            sentenceTokens += addDict['wordToken']\n",
    "            nerTokens += addDict['nerToken']\n",
    "    \n",
    "    # take care of last sentence\n",
    "    # figure out sentence length for padding or truncating\n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "\n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding \n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    # append current data to lists\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    # generate BERT tokens\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    return sentenceTokenList, nerTokenList, sentLengthList, bertSentenceIDs, bertMasks, bertSequenceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceTokenList, nerTokenList, sentLengthList, bertSentenceIDs, bertMasks, bertSequenceIDs = generate_bert_ner_inputs(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(nerTokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 400\n",
    "sentenceLengths= [l for l in sentLengthList]\n",
    "plt.hist(np.array(sentenceLengths), bins=(50))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentLengthList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>nerX</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>nerX</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>nerX</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>nerX</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>nerX</td>\n",
       "      <td>25</td>\n",
       "      <td>6787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag  cat  occurences\n",
       "0    B-EXAMPLE_LABEL    0          49\n",
       "1    B-EXAMPLE_LABEL    1           0\n",
       "2    B-EXAMPLE_LABEL    2           0\n",
       "3    B-EXAMPLE_LABEL    3           0\n",
       "4    B-EXAMPLE_LABEL    4           0\n",
       "..               ...  ...         ...\n",
       "671             nerX   21           0\n",
       "672             nerX   22           0\n",
       "673             nerX   23           0\n",
       "674             nerX   24           0\n",
       "675             nerX   25        6787\n",
       "\n",
       "[676 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>B-OTHER_COMPOUND</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>B-REACTION_PRODUCT</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>B-REACTION_STEP</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>B-REAGENT_CATALYST</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>B-SOLVENT</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>162</td>\n",
       "      <td>B-STARTING_MATERIAL</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>189</td>\n",
       "      <td>B-TEMPERATURE</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>B-TIME</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>243</td>\n",
       "      <td>B-WORKUP</td>\n",
       "      <td>9</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>270</td>\n",
       "      <td>B-YIELD_OTHER</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>297</td>\n",
       "      <td>B-YIELD_PERCENT</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>324</td>\n",
       "      <td>I-OTHER_COMPOUND</td>\n",
       "      <td>12</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>351</td>\n",
       "      <td>I-REACTION_PRODUCT</td>\n",
       "      <td>13</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>378</td>\n",
       "      <td>I-REAGENT_CATALYST</td>\n",
       "      <td>14</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>405</td>\n",
       "      <td>I-SOLVENT</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>432</td>\n",
       "      <td>I-STARTING_MATERIAL</td>\n",
       "      <td>16</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>459</td>\n",
       "      <td>I-TEMPERATURE</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>486</td>\n",
       "      <td>I-TIME</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>513</td>\n",
       "      <td>I-YIELD_OTHER</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>540</td>\n",
       "      <td>I-YIELD_PERCENT</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>567</td>\n",
       "      <td>O</td>\n",
       "      <td>21</td>\n",
       "      <td>4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>594</td>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621</td>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>23</td>\n",
       "      <td>6189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>648</td>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>675</td>\n",
       "      <td>nerX</td>\n",
       "      <td>25</td>\n",
       "      <td>6787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                  tag  cat  occurences\n",
       "0      27     B-OTHER_COMPOUND    1         170\n",
       "1      54   B-REACTION_PRODUCT    2         106\n",
       "2      81      B-REACTION_STEP    3         170\n",
       "3     108   B-REAGENT_CATALYST    4          58\n",
       "4     135            B-SOLVENT    5          61\n",
       "5     162  B-STARTING_MATERIAL    6          88\n",
       "6     189        B-TEMPERATURE    7          73\n",
       "7     216               B-TIME    8          46\n",
       "8     243             B-WORKUP    9         118\n",
       "9     270        B-YIELD_OTHER   10          49\n",
       "10    297      B-YIELD_PERCENT   11          43\n",
       "11    324     I-OTHER_COMPOUND   12         109\n",
       "12    351   I-REACTION_PRODUCT   13         831\n",
       "13    378   I-REAGENT_CATALYST   14          61\n",
       "14    405            I-SOLVENT   15           7\n",
       "15    432  I-STARTING_MATERIAL   16         421\n",
       "16    459        I-TEMPERATURE   17          54\n",
       "17    486               I-TIME   18          45\n",
       "18    513        I-YIELD_OTHER   19          48\n",
       "19    540      I-YIELD_PERCENT   20          43\n",
       "20    567                    O   21        4274\n",
       "21    594             [nerCLS]   22          50\n",
       "22    621             [nerPAD]   23        6189\n",
       "23    648             [nerSEP]   24          50\n",
       "24    675                 nerX   25        6787"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_categories = nerDistribution.loc[(nerDistribution.iloc[:,1:]!=0).all(1)].reset_index()\n",
    "ner_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline: Always picking 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6172732524552282"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = int(nerDistribution.loc[nerDistribution.tag == 'O','occurences']\\\n",
    "                                .reset_index().drop(['index'], axis=1).loc[21])   # Some gymnasics to get the count..\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 22]['occurences'].sum()\n",
    "\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss & Accuracy\n",
    "(from BERT_T5_NER_2_3_030521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom loss function because we only want to optimize for the labels that we actually had in the text, not the extra ones like '[nerPAD]', etc. Our cost function is therefore derived from sparse_categorical_crossentropy, but we choose to modify the function a bit: we want to mask out all tokens that have a token id larger or equal to 22, corresponding to the extra tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 22)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 22)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 21)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(26, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 400, 768), dtype=tf.float32, name=None), name='tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0', description=\"created by layer 'tf_bert_model'\")\n",
      "pred:  KerasTensor(type_spec=TensorSpec(shape=(None, 400, 26), dtype=tf.float32, name=None), name='ner/Softmax:0', description=\"created by layer 'ner'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 400, 256)     196864      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 400, 256)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 400, 26)      6682        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,513,818\n",
      "Trainable params: 108,513,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/8\n",
      "2/2 [==============================] - 33s 11s/step - loss: 3.4549 - custom_acc_orig_tokens: 0.2065 - custom_acc_orig_non_other_tokens: 0.0566 - val_loss: 1.8938 - val_custom_acc_orig_tokens: 0.6125 - val_custom_acc_orig_non_other_tokens: 0.0855\n",
      "Epoch 2/8\n",
      "2/2 [==============================] - 28s 9s/step - loss: 1.8931 - custom_acc_orig_tokens: 0.6552 - custom_acc_orig_non_other_tokens: 0.0608 - val_loss: 1.4099 - val_custom_acc_orig_tokens: 0.6491 - val_custom_acc_orig_non_other_tokens: 0.1366\n",
      "Epoch 3/8\n",
      "2/2 [==============================] - 26s 9s/step - loss: 1.4826 - custom_acc_orig_tokens: 0.6780 - custom_acc_orig_non_other_tokens: 0.1519 - val_loss: 1.3177 - val_custom_acc_orig_tokens: 0.7104 - val_custom_acc_orig_non_other_tokens: 0.2874\n",
      "Epoch 4/8\n",
      "2/2 [==============================] - 26s 8s/step - loss: 1.4468 - custom_acc_orig_tokens: 0.6974 - custom_acc_orig_non_other_tokens: 0.1657 - val_loss: 1.2049 - val_custom_acc_orig_tokens: 0.7394 - val_custom_acc_orig_non_other_tokens: 0.3587\n",
      "Epoch 5/8\n",
      "2/2 [==============================] - 25s 8s/step - loss: 1.3371 - custom_acc_orig_tokens: 0.7175 - custom_acc_orig_non_other_tokens: 0.2652 - val_loss: 1.0533 - val_custom_acc_orig_tokens: 0.7403 - val_custom_acc_orig_non_other_tokens: 0.3646\n",
      "Epoch 6/8\n",
      "2/2 [==============================] - 24s 8s/step - loss: 1.1821 - custom_acc_orig_tokens: 0.7273 - custom_acc_orig_non_other_tokens: 0.2482 - val_loss: 0.9298 - val_custom_acc_orig_tokens: 0.7292 - val_custom_acc_orig_non_other_tokens: 0.3409\n",
      "Epoch 7/8\n",
      "2/2 [==============================] - 24s 8s/step - loss: 1.0595 - custom_acc_orig_tokens: 0.7367 - custom_acc_orig_non_other_tokens: 0.1752 - val_loss: 0.8679 - val_custom_acc_orig_tokens: 0.7085 - val_custom_acc_orig_non_other_tokens: 0.2957\n",
      "Epoch 8/8\n",
      "2/2 [==============================] - 24s 8s/step - loss: 0.9739 - custom_acc_orig_tokens: 0.7181 - custom_acc_orig_non_other_tokens: 0.2179 - val_loss: 0.8193 - val_custom_acc_orig_tokens: 0.7235 - val_custom_acc_orig_non_other_tokens: 0.3456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff254f6ddf0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ner_model(max_length + 1,train_layers=0,optimizer='adam')\n",
    "\n",
    "# Instantiate variables\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k}),\n",
    "    epochs=8,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
