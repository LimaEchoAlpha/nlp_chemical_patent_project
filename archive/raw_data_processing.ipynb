{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       and labels it with BIO tags based on cleaned offsets \n",
    "       from its .ann file\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_sentence = patent sentence with annotations for each entity\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    entity_id = 'X'\n",
    "    \n",
    "    sentence = [[x.strip(punctuation), label, entity_id] \n",
    "                for x in full_text[:end].replace('\\n', ' ').split(' ')\n",
    "                if x.strip(punctuation)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        entity_id = offsets[i][2]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            \n",
    "            if label == 'O':            \n",
    "                terms = [[x.strip(punctuation), label, entity_id] \n",
    "                         for x in full_text[start:end].replace('\\n', ' ').split(' ') \n",
    "                         if x.strip(punctuation)]\n",
    "                sentence.extend(terms)\n",
    "            \n",
    "            else:\n",
    "                terms = [x \n",
    "                         for x in full_text[start:end].replace('\\n', ' ').split(' ') \n",
    "                         if x]\n",
    "                bio_terms = [[terms[0], 'B-' + label, entity_id]]\n",
    "                if len(terms) > 1:\n",
    "                    bio_terms.extend([[terms[i], 'I-' + label, entity_id] for i in range(1, len(terms))])\n",
    "                sentence.extend(bio_terms)\n",
    "        \n",
    "        else:\n",
    "            terms = [[x.strip(punctuation), label, entity_id] \n",
    "                     for x in full_text[start:].replace('\\n', ' ').split(' ') \n",
    "                     if x.strip(punctuation)]  \n",
    "            sentence.extend(terms)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file for use in NER task\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    sentences = []\n",
    "    corrections = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        sentences.append([f'SENTENCE: {file[-4:]}'])\n",
    "        corrections.append([f'SENTENCE: {file[-4:]}'])\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        corrections.append(file_corrections)\n",
    "        \n",
    "        ann_sentence = process_txt(f'{file}.txt', cleaned_offsets)\n",
    "        sentences.extend(ann_sentence)\n",
    "        \n",
    "        #sentences.append([f'COUNT: {len(ann_sentence)}'])\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(sentences)\n",
    "    \n",
    "    with open(f'{output_path}_corrections.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(corrections)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'raw_data/sample/0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 194\\n3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione (racemate)\\n813 mg (1.84 mmol) of the compound from Example 243A were dissolved in 40 ml of dioxane, and 461 mg (2.76 mmol) of CDI were added. The mixture was stirred at RT for 16 h. The reaction solution was then concentrated on a rotary evaporator. The residue was dissolved in 15 ml of DMSO and this solution was purified by means of preparative HPLC (Method 14). Combination of the product fractions and freeze-drying gave 383 mg (42% of theory) of the title compound'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'OTHER_COMPOUND 417 421', 'DMSO'],\n",
       " ['T1', 'TIME 305 309', '16 h'],\n",
       " ['T2', 'REACTION_PRODUCT 585 599', 'title compound'],\n",
       " ['T11', 'REACTION_STEP 550 554', 'gave'],\n",
       " ['T3', 'YIELD_PERCENT 563 566', '42%']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'T']\n",
    "ann[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_corrections = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'EXAMPLE_LABEL', 'T8'),\n",
       " (11, 'O', 'X'),\n",
       " (12, 'REACTION_PRODUCT', 'T6'),\n",
       " (139, 'O', 'X'),\n",
       " (166, 'STARTING_MATERIAL', 'T9')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = process_txt(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Example', 'O', 'X'],\n",
       " ['194', 'B-EXAMPLE_LABEL', 'T8'],\n",
       " ['3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione',\n",
       "  'B-REACTION_PRODUCT',\n",
       "  'T6'],\n",
       " ['(racemate)', 'I-REACTION_PRODUCT', 'T6'],\n",
       " ['813', 'O', 'X']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.046 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SENTENCE: 0000\\n',\n",
       " 'Example\\tO\\tX\\n',\n",
       " '194\\tB-EXAMPLE_LABEL\\tT8\\n',\n",
       " '3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione\\tB-REACTION_PRODUCT\\tT6\\n',\n",
       " '(racemate)\\tI-REACTION_PRODUCT\\tT6\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ner_files([test_path], 'raw_data/test')\n",
    "with io.open('raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = 'raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = 'raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = 'raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = 'raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 7.175 seconds\n",
      "Number of train sentences: 900\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "# print(len(filepath_train))\n",
    "# filepath_train[:5]\n",
    "\n",
    "output_train = 'data/ee_ner_train'\n",
    "generate_ner_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5.112 seconds\n",
      "Number of train sentences: 225\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = 'data/ee_ner_dev'\n",
    "generate_ner_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 8.986 seconds\n",
      "Number of train sentences: 375\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = 'data/ee_ner_test'\n",
    "generate_ner_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.261 seconds\n",
      "Number of train sentences: 50\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = 'raw_data/sample'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = 'data/ee_ner_sample'\n",
    "generate_ner_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
