{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count NER document length after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_labeler(chunks):\n",
    "    \"\"\"Helper function that further processes annotated chunks from ann_chunker()\n",
    "       Tokenizes the chunks and applies BIO labels to each token\n",
    "       \n",
    "       Inputs:\n",
    "       chunks = list of annotated chunks based on .ann file offsets\n",
    "                format: (chunk, label)\n",
    "       \n",
    "       Output:\n",
    "       bio_doc = document transformed into a list of tokens with bio labels\"\"\"\n",
    "    \n",
    "    bio_tokens = []\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label = tup\n",
    "        \n",
    "        if label == 'O':\n",
    "            if chunk[:1] == '\\n':\n",
    "                bio_tokens.append([])\n",
    "            sentences = sent_tokenize(chunk.strip())\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        bio_tokens.append([x, label])\n",
    "                        if x == '.':\n",
    "                            bio_tokens.append([])\n",
    "                \n",
    "        else:\n",
    "            tokens = [x for x in word_tokenize(chunk)]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    bio_tokens.append([tokens[i], 'B-' + label])\n",
    "                else:\n",
    "                    bio_tokens.append([tokens[i], 'I-' + label])\n",
    "    \n",
    "    return bio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_doc_length(filepaths):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and counts the number of tokens for each after processing\n",
    "       \n",
    "       Input:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       \n",
    "       Output:\n",
    "       doc_lengths = dictionary of document lengths\"\"\"\n",
    "    \n",
    "    doc_lengths = {}\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        bio_tokens = bio_labeler(chunks)\n",
    "        \n",
    "        #doc_lengths.update({file[-4:]: len(chunks)})\n",
    "        doc_lengths.update({file[-4:]: len(bio_tokens)})\n",
    "    \n",
    "    return doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count document lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 50\n",
      "Minimum snippet length: 48\n",
      "Maximum snippet length: 318\n"
     ]
    }
   ],
   "source": [
    "# sample set\n",
    "path_sample = 'raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "sample_lengths = count_doc_length(filepath_sample)\n",
    "\n",
    "print(f'Number of files: {len(sample_lengths.keys())}')\n",
    "print(f'Minimum snippet length: {min(sample_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(sample_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 900\n",
      "Minimum snippet length: 36\n",
      "Maximum snippet length: 1301\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "path_train = 'raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "train_lengths = count_doc_length(filepath_train)\n",
    "\n",
    "print(f'Number of files: {len(train_lengths.keys())}')\n",
    "print(f'Minimum snippet length: {min(train_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(train_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 3, ['1123', '0344', '0311']\n",
      "Snippets larger than BERT base: 11, ['1378', '1123', '0729', '0344', '0110', '1122', '1307', '0311', '0242', '0929', '0532']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "train_large = [key for key, value in train_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(train_large)}, {train_large}')\n",
    "\n",
    "train_base = [key for key, value in train_lengths.items() if value > 500]\n",
    "print(f'Snippets larger than BERT base: {len(train_base)}, {train_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT base: 17, ['1378', '0762', '1123', '1289', '0729', '0344', '0110', '1122', '0411', '0129', '1307', '0311', '0810', '0242', '0929', '0951', '0532']\n"
     ]
    }
   ],
   "source": [
    "train_other = [key for key, value in train_lengths.items() if value > 400]\n",
    "print(f'Snippets larger than BERT base: {len(train_other)}, {train_other}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 225\n",
      "Minimum snippet length: 43\n",
      "Maximum snippet length: 652\n"
     ]
    }
   ],
   "source": [
    "# dev set\n",
    "path_dev = 'raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "dev_lengths = count_doc_length(filepath_dev)\n",
    "\n",
    "print(f'Number of files: {len(dev_lengths.keys())}')\n",
    "print(f'Minimum snippet length: {min(dev_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(dev_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 0, []\n",
      "Snippets larger than BERT base: 1, ['0389']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "dev_large = [key for key, value in dev_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(dev_large)}, {dev_large}')\n",
    "\n",
    "dev_base = [key for key, value in dev_lengths.items() if value > 500]\n",
    "print(f'Snippets larger than BERT base: {len(dev_base)}, {dev_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 375\n",
      "Minimum snippet length: 43\n",
      "Maximum snippet length: 1155\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "path_test = 'raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "path_test_ann = 'raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "test_lengths = count_doc_length(filepath_test)\n",
    "\n",
    "print(f'Number of files: {len(test_lengths.keys())}')\n",
    "print(f'Minimum snippet length: {min(test_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(test_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 1, ['7980']\n",
      "Snippets larger than BERT base: 3, ['7980', '6846', '1283']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "test_large = [key for key, value in test_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(test_large)}, {test_large}')\n",
    "\n",
    "test_base = [key for key, value in test_lengths.items() if value > 500]\n",
    "print(f'Snippets larger than BERT base: {len(test_base)}, {test_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
