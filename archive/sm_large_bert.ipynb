{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sentencepiece\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/sample'\n",
    "filename = 'sample_ner.csv'\n",
    "full_path = f'{path}/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sample/sample_ner.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 16409, 26671, 21720, 124, 118, 2181, 12809, 3818, 7777, 118, 126, 118, 1899, 18873, 118, 122, 118, 113, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Example 194 3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione (racemate)\", max_length=20, padding='max_length', truncation=True)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Example 194 3 - Isobutyl - 5 - methyl - 1 - ( [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', '##va', '##por', '##ator']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('evaporator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate NER label list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER labels. \n",
    "    Note that the word can be tokenized to two or more tokens. \n",
    "    Correspondingly, we add - for now - custom 'X' tokens to the labels \n",
    "    in order to maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \n",
    "    *Function modified from BERT_T5_NER_2_3_030521\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponding to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    return addDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_ner_inputs(full_path, max_length=400):\n",
    "    \"\"\"\n",
    "    Read the file line by line and construct snippets (\"sentences\"). \n",
    "    A snippet end is marked by 'SNIPPET: [snippet id]' in the next row.\n",
    "    \n",
    "    Also, cap snippet length using max_length. \n",
    "    Snippets which are shorter than max_length are padded. \n",
    "    All snippets end with a [SEP] token, padded or not.\n",
    "    \n",
    "    arguments: full path of input file, max length of snippet\n",
    "    returns: lists for BERT inputs (bertSentenceIDs, bertMasks, bertSequenceIDs)\n",
    "             and the inputs before BERT tokenization (sentenceList, nerTokenList, sentLengthList)\n",
    "    \n",
    "    *Function modified from BERT_T5_NER_2_3_030521\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "\n",
    "    # lists for sentences, tokens, labels, etc. \n",
    "    sentenceTokenList = []\n",
    "    nerTokenList = []\n",
    "    sentLengthList = []\n",
    "\n",
    "    # lists for BERT input\n",
    "    bertSentenceIDs = []\n",
    "    bertMasks = []\n",
    "    bertSequenceIDs = []\n",
    "\n",
    "    # always start with [CLS] tokens\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    for line in text:\n",
    "        \n",
    "        parsed_line = line.strip().split('\\t')\n",
    "        \n",
    "        # if new sentence starts\n",
    "        if parsed_line[0][:8] == 'SNIPPET:':\n",
    "            \n",
    "            if len(sentenceTokens) > 1:\n",
    "            \n",
    "                # figure out sentence length for padding or truncating\n",
    "                sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "                sentLengthList.append(sentenceLength)\n",
    "\n",
    "                # Create space for at least a final '[SEP]' token\n",
    "                if sentenceLength >= max_length - 1: \n",
    "                    sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "                    nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "                # add a ['SEP'] token and padding \n",
    "                sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "                nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "                # append current data to lists\n",
    "                sentenceTokenList.append(sentenceTokens)\n",
    "                nerTokenList.append(nerTokens)\n",
    "\n",
    "                # generate BERT tokens\n",
    "                bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "                bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "                bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "                # initialize new sentence\n",
    "                sentenceTokens = ['[CLS]']\n",
    "                nerTokens = ['[nerCLS]']\n",
    "                \n",
    "        elif parsed_line[0]:            \n",
    "            word = parsed_line[0]\n",
    "            ner = parsed_line[1]\n",
    "            \n",
    "            addDict = addWord(word, ner)\n",
    "            sentenceTokens += addDict['wordToken']\n",
    "            nerTokens += addDict['nerToken']\n",
    "    \n",
    "    # take care of last sentence\n",
    "    # figure out sentence length for padding or truncating\n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "\n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding \n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    # append current data to lists\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    # generate BERT tokens\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    return sentenceTokenList, nerTokenList, sentLengthList, bertSentenceIDs, bertMasks, bertSequenceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceTokenList, nerTokenList, sentLengthList, bertSentenceIDs, bertMasks, bertSequenceIDs = generate_bert_ner_inputs(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nerTokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 400\n",
    "sentenceLengths= [l for l in sentLengthList]\n",
    "plt.hist(np.array(sentenceLengths), bins=(50))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentLengthList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-EXAMPLE_LABEL</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>nerX</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>nerX</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>nerX</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>nerX</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>nerX</td>\n",
       "      <td>25</td>\n",
       "      <td>8069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag  cat  occurences\n",
       "0    B-EXAMPLE_LABEL    0          49\n",
       "1    B-EXAMPLE_LABEL    1           0\n",
       "2    B-EXAMPLE_LABEL    2           0\n",
       "3    B-EXAMPLE_LABEL    3           0\n",
       "4    B-EXAMPLE_LABEL    4           0\n",
       "..               ...  ...         ...\n",
       "671             nerX   21           0\n",
       "672             nerX   22           0\n",
       "673             nerX   23           0\n",
       "674             nerX   24           0\n",
       "675             nerX   25        8069\n",
       "\n",
       "[676 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>B-OTHER_COMPOUND</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>B-REACTION_PRODUCT</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>B-REACTION_STEP</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>B-REAGENT_CATALYST</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>B-SOLVENT</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>162</td>\n",
       "      <td>B-STARTING_MATERIAL</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>189</td>\n",
       "      <td>B-TEMPERATURE</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>B-TIME</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>243</td>\n",
       "      <td>B-WORKUP</td>\n",
       "      <td>9</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>270</td>\n",
       "      <td>B-YIELD_OTHER</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>297</td>\n",
       "      <td>B-YIELD_PERCENT</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>324</td>\n",
       "      <td>I-OTHER_COMPOUND</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>351</td>\n",
       "      <td>I-REACTION_PRODUCT</td>\n",
       "      <td>13</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>378</td>\n",
       "      <td>I-REAGENT_CATALYST</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>405</td>\n",
       "      <td>I-SOLVENT</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>432</td>\n",
       "      <td>I-STARTING_MATERIAL</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>459</td>\n",
       "      <td>I-TEMPERATURE</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>486</td>\n",
       "      <td>I-TIME</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>513</td>\n",
       "      <td>I-YIELD_OTHER</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>540</td>\n",
       "      <td>I-YIELD_PERCENT</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>567</td>\n",
       "      <td>O</td>\n",
       "      <td>21</td>\n",
       "      <td>4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>594</td>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621</td>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>23</td>\n",
       "      <td>6189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>648</td>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>675</td>\n",
       "      <td>nerX</td>\n",
       "      <td>25</td>\n",
       "      <td>8069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                  tag  cat  occurences\n",
       "0      27     B-OTHER_COMPOUND    1         170\n",
       "1      54   B-REACTION_PRODUCT    2         106\n",
       "2      81      B-REACTION_STEP    3         170\n",
       "3     108   B-REAGENT_CATALYST    4          58\n",
       "4     135            B-SOLVENT    5          61\n",
       "5     162  B-STARTING_MATERIAL    6          88\n",
       "6     189        B-TEMPERATURE    7          73\n",
       "7     216               B-TIME    8          46\n",
       "8     243             B-WORKUP    9         118\n",
       "9     270        B-YIELD_OTHER   10          49\n",
       "10    297      B-YIELD_PERCENT   11          43\n",
       "11    324     I-OTHER_COMPOUND   12          55\n",
       "12    351   I-REACTION_PRODUCT   13          52\n",
       "13    378   I-REAGENT_CATALYST   14          39\n",
       "14    405            I-SOLVENT   15           3\n",
       "15    432  I-STARTING_MATERIAL   16          40\n",
       "16    459        I-TEMPERATURE   17          54\n",
       "17    486               I-TIME   18          45\n",
       "18    513        I-YIELD_OTHER   19          48\n",
       "19    540      I-YIELD_PERCENT   20           1\n",
       "20    567                    O   21        4274\n",
       "21    594             [nerCLS]   22          50\n",
       "22    621             [nerPAD]   23        6189\n",
       "23    648             [nerSEP]   24          50\n",
       "24    675                 nerX   25        8069"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_categories = nerDistribution.loc[(nerDistribution.iloc[:,1:]!=0).all(1)].reset_index()\n",
    "ner_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline: Always picking 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575327897908543"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = int(nerDistribution.loc[nerDistribution.tag == 'O','occurences']\\\n",
    "                                .reset_index().drop(['index'], axis=1).loc[21])   # Some gymnasics to get the count..\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 22]['occurences'].sum()\n",
    "\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  101,   113, 16409, ...,   113,   102,     0],\n",
       "        [  101, 16409, 26671, ...,     0,     0,     0],\n",
       "        [  101, 16409, 26671, ...,   110,   102,     0],\n",
       "        ...,\n",
       "        [  101,   156, 26588, ...,     0,     0,     0],\n",
       "        [  101, 16409, 26671, ...,     0,     0,     0],\n",
       "        [  101, 16409, 26671, ...,     0,     0,     0]]),\n",
       " array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_train_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss & Accuracy\n",
    "(from BERT_T5_NER_2_3_030521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom loss function because we only want to optimize for the labels that we actually had in the text, not the extra ones like '[nerPAD]', etc. Our cost function is therefore derived from sparse_categorical_crossentropy, but we choose to modify the function a bit: we want to mask out all tokens that have a token id larger or equal to 22, corresponding to the extra tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 22)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 22)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 21)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-large-cased')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(26, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e374ddf647e74ac5a9c669994aa46929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1460062736.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 400, 1024), dtype=tf.float32, name=None), name='tf_bert_model/bert/encoder/layer_._23/output/LayerNorm/batchnorm/add_1:0', description=\"created by layer 'tf_bert_model'\")\n",
      "pred:  KerasTensor(type_spec=TensorSpec(shape=(None, 400, 26), dtype=tf.float32, name=None), name='ner/Softmax:0', description=\"created by layer 'ner'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 333579264   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 400, 256)     262400      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 400, 256)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 400, 26)      6682        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 333,848,346\n",
      "Trainable params: 333,848,346\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-70944aca1b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Instantiate variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbert_inputs_train_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_train_k\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ner_model(max_length + 1,train_layers=0,optimizer='adam')\n",
    "\n",
    "# Instantiate variables\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k}),\n",
    "    epochs=8,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
