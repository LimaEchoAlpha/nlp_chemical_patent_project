{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_labeler(chunks):\n",
    "    \"\"\"Helper function that further processes annotated chunks from ann_chunker()\n",
    "       Tokenizes the chunks and applies BIO labels to each token\n",
    "       \n",
    "       Inputs:\n",
    "       chunks = list of annotated chunks based on .ann file offsets\n",
    "                format: (chunk, label)\n",
    "       \n",
    "       Output:\n",
    "       bio_doc = document transformed into a list of tokens with bio labels\"\"\"\n",
    "    \n",
    "    bio_tokens = []\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label = tup\n",
    "        \n",
    "        if label == 'O':\n",
    "            sentences = sent_tokenize(chunk.strip())\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        bio_tokens.append([x, label])\n",
    "                \n",
    "        else:\n",
    "            tokens = [x for x in chunk.split(' ')]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    bio_tokens.append([tokens[i], 'B-' + label])\n",
    "                else:\n",
    "                    bio_tokens.append([tokens[i], 'I-' + label])\n",
    "    \n",
    "    return bio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file in CoNLL 2003 format (for use in NER task)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    docs = []\n",
    "    corrections = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        docs.append([f'SNIPPET: {file[-4:]}'])\n",
    "        corrections.append([f'SNIPPET: {file[-4:]}'])\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        corrections.append(file_corrections)\n",
    "        \n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        bio_tokens = bio_labeler(chunks)\n",
    "        docs.extend(bio_tokens)\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(docs)\n",
    "    \n",
    "    with open(f'{output_path}_corrections.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(corrections)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_doc_length(filepaths):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and counts the number of tokens for each after processing\n",
    "       \n",
    "       Input:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       \n",
    "       Output:\n",
    "       doc_lengths = dictionary of document lengths\"\"\"\n",
    "    \n",
    "    doc_lengths = {}\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        bio_tokens = bio_labeler(chunks)\n",
    "        \n",
    "        #doc_lengths.update({file[-4:]: len(chunks)})\n",
    "        doc_lengths.update({file[-4:]: len(bio_tokens)})\n",
    "    \n",
    "    return doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../raw_data/sample_ee/0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 194\\n3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione (racemate)\\n813 mg (1.84 mmol) of the compound from Example 243A were dissolved in 40 ml of dioxane, and 461 mg (2.76 mmol) of CDI were added. The mixture was stirred at RT for 16 h. The reaction solution was then concentrated on a rotary evaporator. The residue was dissolved in 15 ml of DMSO and this solution was purified by means of preparative HPLC (Method 14). Combination of the product fractions and freeze-drying gave 383 mg (42% of theory) of the title compound'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'OTHER_COMPOUND 417 421', 'DMSO'],\n",
       " ['T1', 'TIME 305 309', '16 h'],\n",
       " ['T2', 'REACTION_PRODUCT 585 599', 'title compound']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'T']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_corrections = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'EXAMPLE_LABEL', 'T8'), (11, 'O', 'X'), (12, 'REACTION_PRODUCT', 'T6')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Example ', 'O'), ('194', 'EXAMPLE_LABEL'), ('\\n', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Example', 'O'],\n",
       " ['194', 'B-EXAMPLE_LABEL'],\n",
       " ['3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione',\n",
       "  'B-REACTION_PRODUCT']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_doc = bio_labeler(trial_sentence)\n",
    "trial_doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trial_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.051 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SNIPPET: 0000\\n', 'Example\\tO\\n', '194\\tB-EXAMPLE_LABEL\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ner_files([test_path], '../raw_data/test')\n",
    "with io.open('../raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SNIPPET:'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.579 seconds\n",
      "Number of train snippets: 50\n",
      "Minimum snippet length: 38\n",
      "Maximum snippet length: 212\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = '../raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = '../data/sample/sample_ner'\n",
    "generate_ner_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8] == 'SNIPPET:']\n",
    "print(f'Number of train snippets: {len(check)}')\n",
    "\n",
    "sample_lengths = count_doc_length(filepath_sample)\n",
    "print(f'Minimum snippet length: {min(sample_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(sample_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = '../raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = '../raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = '../raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = '../raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 15.154 seconds\n",
      "Number of train sentences: 900\n",
      "Minimum snippet length: 32\n",
      "Maximum snippet length: 1053\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/ner/ee_ner_train'\n",
    "generate_ner_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8] == 'SNIPPET:']\n",
    "print(f'Number of train sentences: {len(check)}')\n",
    "\n",
    "train_lengths = count_doc_length(filepath_train)\n",
    "print(f'Minimum snippet length: {min(train_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(train_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 1, ['0344']\n",
      "Snippets larger than BERT base: 10, ['1122', '1123', '0929', '1307', '0729', '0311', '1378', '0110', '0532', '0344']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "train_large = [key for key, value in train_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(train_large)}, {train_large}')\n",
    "\n",
    "train_base = [key for key, value in train_lengths.items() if value > 510]\n",
    "print(f'Snippets larger than BERT base: {len(train_base)}, {train_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT base: 3, ['1123', '0311', '0344']\n"
     ]
    }
   ],
   "source": [
    "train_other = [key for key, value in train_lengths.items() if value > 900]\n",
    "print(f'Snippets larger than BERT base: {len(train_other)}, {train_other}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 4.394 seconds\n",
      "Number of train sentences: 225\n",
      "Minimum snippet length: 33\n",
      "Maximum snippet length: 542\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/ner/ee_ner_dev'\n",
    "generate_ner_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8] == 'SNIPPET:']\n",
    "print(f'Number of train sentences: {len(check)}')\n",
    "\n",
    "dev_lengths = count_doc_length(filepath_dev)\n",
    "print(f'Minimum snippet length: {min(dev_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(dev_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 0, []\n",
      "Snippets larger than BERT base: 1, ['0389']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "dev_large = [key for key, value in dev_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(dev_large)}, {dev_large}')\n",
    "\n",
    "dev_base = [key for key, value in dev_lengths.items() if value > 500]\n",
    "print(f'Snippets larger than BERT base: {len(dev_base)}, {dev_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 6.896 seconds\n",
      "Number of train sentences: 375\n",
      "Minimum snippet length: 32\n",
      "Maximum snippet length: 1009\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = '../data/ner/ee_ner_test'\n",
    "generate_ner_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8] == 'SNIPPET:']\n",
    "print(f'Number of train sentences: {len(check)}')\n",
    "\n",
    "test_lengths = count_doc_length(filepath_test)\n",
    "print(f'Minimum snippet length: {min(test_lengths.values())}')\n",
    "print(f'Maximum snippet length: {max(test_lengths.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets larger than BERT large: 0, []\n",
      "Snippets larger than BERT base: 3, ['7980', '6846', '1283']\n"
     ]
    }
   ],
   "source": [
    "# how many documents are larger than BERT base and large?\n",
    "test_large = [key for key, value in test_lengths.items() if value > 1022]\n",
    "print(f'Snippets larger than BERT large: {len(test_large)}, {test_large}')\n",
    "\n",
    "test_base = [key for key, value in test_lengths.items() if value > 500]\n",
    "print(f'Snippets larger than BERT base: {len(test_base)}, {test_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
