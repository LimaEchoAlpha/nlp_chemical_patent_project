{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_labeler(chunks):\n",
    "    \"\"\"Helper function that further processes annotated chunks from ann_chunker()\n",
    "       Tokenizes the chunks and applies BIO labels to each token\n",
    "       \n",
    "       Inputs:\n",
    "       chunks = list of annotated chunks based on .ann file offsets\n",
    "                format: (chunk, label)\n",
    "       \n",
    "       Output:\n",
    "       bio_doc = document transformed into a list of tokens with bio labels\"\"\"\n",
    "    \n",
    "    bio_tokens = []\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label = tup\n",
    "        \n",
    "        if label == 'O':\n",
    "            if chunk[:1] == '\\n':\n",
    "                bio_tokens.append([])\n",
    "            sentences = sent_tokenize(chunk.strip())\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        bio_tokens.append([x, label])\n",
    "                        if x == '.':\n",
    "                            bio_tokens.append([])\n",
    "                \n",
    "        else:\n",
    "            tokens = [x for x in word_tokenize(chunk)]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    bio_tokens.append([tokens[i], 'B-' + label])\n",
    "                else:\n",
    "                    bio_tokens.append([tokens[i], 'I-' + label])\n",
    "    \n",
    "    return bio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file in CoNLL 2003 format (for use in NER task)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    docs = []\n",
    "    corrections = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        docs.append(['-DOCSTART- -X- O O'])\n",
    "        corrections.append([f'SENTENCE: {file[-4:]}'])\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        corrections.append(file_corrections)\n",
    "        \n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        bio_tokens = bio_labeler(chunks)\n",
    "        docs.extend(bio_tokens)\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(docs)\n",
    "    \n",
    "    with open(f'{output_path}_corrections.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(corrections)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'raw_data/sample_ee/0001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16.8: [5-(2,3-difluorophenyl)-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl]-acetic acid\\n2.3 ml (2.3 mmol) of a 1N aqueous solution of lithium hydroxide is added to a solution of 475 mg (1.5 mmol) of [5-(2,3-difluorophenyl)-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl]-methyl acetate in 15 ml of tetrahydrofuran and 3 mL of water. The reaction mixture is stirred at room temperature for 2 hours, and then adjusted to pH6 by adding 4 ml of 1N aqueous solution of acetic acid. The product is extracted with ethyl acetate. The organic phase is washed with water and then with a saturated aqueous solution of sodium chloride, dried over magnesium sulphate, filtered and concentrated under vacuum. 400 mg (88%) of [5-(2,3-difluorophenyl)-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl]-acetic acid is obtained in the form of a white solid.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'REAGENT_CATALYST 332 337', 'water'],\n",
       " ['T19', 'WORKUP 549 555', 'washed'],\n",
       " ['T1', 'OTHER_COMPOUND 450 457', 'aqueous']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'T']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_corrections = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'EXAMPLE_LABEL', 'T10'), (4, 'O', 'X'), (6, 'REACTION_PRODUCT', 'T13')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'O'), ('16.8', 'EXAMPLE_LABEL'), (': ', 'O')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['16.8', 'B-EXAMPLE_LABEL'], [':', 'O'], ['[', 'B-REACTION_PRODUCT']]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_doc = bio_labeler(trial_sentence)\n",
    "trial_doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.079 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['-DOCSTART- -X- O O\\n',\n",
       " '16.8\\tB-EXAMPLE_LABEL\\n',\n",
       " ':\\tO\\n',\n",
       " '[\\tB-REACTION_PRODUCT\\n',\n",
       " '5-\\tI-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " '2,3-difluorophenyl\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " '-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl\\tI-REACTION_PRODUCT\\n',\n",
       " ']\\tI-REACTION_PRODUCT\\n',\n",
       " '-acetic\\tI-REACTION_PRODUCT\\n',\n",
       " 'acid\\tI-REACTION_PRODUCT\\n',\n",
       " '\\n',\n",
       " '2.3\\tO\\n',\n",
       " 'ml\\tO\\n',\n",
       " '(\\tO\\n',\n",
       " '2.3\\tO\\n',\n",
       " 'mmol\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'a\\tO\\n',\n",
       " '1N\\tO\\n',\n",
       " 'aqueous\\tB-REAGENT_CATALYST\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'lithium\\tB-REAGENT_CATALYST\\n',\n",
       " 'hydroxide\\tI-REAGENT_CATALYST\\n',\n",
       " 'is\\tO\\n',\n",
       " 'added\\tB-REACTION_STEP\\n',\n",
       " 'to\\tO\\n',\n",
       " 'a\\tO\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " '475\\tO\\n',\n",
       " 'mg\\tO\\n',\n",
       " '(\\tO\\n',\n",
       " '1.5\\tO\\n',\n",
       " 'mmol\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " '[\\tB-STARTING_MATERIAL\\n',\n",
       " '5-\\tI-STARTING_MATERIAL\\n',\n",
       " '(\\tI-STARTING_MATERIAL\\n',\n",
       " '2,3-difluorophenyl\\tI-STARTING_MATERIAL\\n',\n",
       " ')\\tI-STARTING_MATERIAL\\n',\n",
       " '-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl\\tI-STARTING_MATERIAL\\n',\n",
       " ']\\tI-STARTING_MATERIAL\\n',\n",
       " '-methyl\\tI-STARTING_MATERIAL\\n',\n",
       " 'acetate\\tI-STARTING_MATERIAL\\n',\n",
       " 'in\\tO\\n',\n",
       " '15\\tO\\n',\n",
       " 'ml\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'tetrahydrofuran\\tB-SOLVENT\\n',\n",
       " 'and\\tO\\n',\n",
       " '3\\tO\\n',\n",
       " 'mL\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'water\\tB-REAGENT_CATALYST\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'reaction\\tO\\n',\n",
       " 'mixture\\tO\\n',\n",
       " 'is\\tO\\n',\n",
       " 'stirred\\tB-REACTION_STEP\\n',\n",
       " 'at\\tO\\n',\n",
       " 'room\\tB-TEMPERATURE\\n',\n",
       " 'temperature\\tI-TEMPERATURE\\n',\n",
       " 'for\\tO\\n',\n",
       " '2\\tB-TIME\\n',\n",
       " 'hours\\tI-TIME\\n',\n",
       " ',\\tO\\n',\n",
       " 'and\\tO\\n',\n",
       " 'then\\tO\\n',\n",
       " 'adjusted\\tO\\n',\n",
       " 'to\\tO\\n',\n",
       " 'pH6\\tO\\n',\n",
       " 'by\\tO\\n',\n",
       " 'adding\\tB-WORKUP\\n',\n",
       " '4\\tO\\n',\n",
       " 'ml\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " '1N\\tO\\n',\n",
       " 'aqueous\\tB-OTHER_COMPOUND\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'acetic\\tB-OTHER_COMPOUND\\n',\n",
       " 'acid\\tI-OTHER_COMPOUND\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'product\\tO\\n',\n",
       " 'is\\tO\\n',\n",
       " 'extracted\\tB-WORKUP\\n',\n",
       " 'with\\tO\\n',\n",
       " 'ethyl\\tB-OTHER_COMPOUND\\n',\n",
       " 'acetate\\tI-OTHER_COMPOUND\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'organic\\tO\\n',\n",
       " 'phase\\tO\\n',\n",
       " 'is\\tO\\n',\n",
       " 'washed\\tB-WORKUP\\n',\n",
       " 'with\\tO\\n',\n",
       " 'water\\tB-OTHER_COMPOUND\\n',\n",
       " 'and\\tO\\n',\n",
       " 'then\\tO\\n',\n",
       " 'with\\tO\\n',\n",
       " 'a\\tO\\n',\n",
       " 'saturated\\tO\\n',\n",
       " 'aqueous\\tB-OTHER_COMPOUND\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'sodium\\tB-OTHER_COMPOUND\\n',\n",
       " 'chloride\\tI-OTHER_COMPOUND\\n',\n",
       " ',\\tO\\n',\n",
       " 'dried\\tB-WORKUP\\n',\n",
       " 'over\\tO\\n',\n",
       " 'magnesium\\tB-OTHER_COMPOUND\\n',\n",
       " 'sulphate\\tI-OTHER_COMPOUND\\n',\n",
       " ',\\tO\\n',\n",
       " 'filtered\\tO\\n',\n",
       " 'and\\tO\\n',\n",
       " 'concentrated\\tO\\n',\n",
       " 'under\\tO\\n',\n",
       " 'vacuum\\tO\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " '400\\tB-YIELD_OTHER\\n',\n",
       " 'mg\\tI-YIELD_OTHER\\n',\n",
       " '(\\tO\\n',\n",
       " '88\\tB-YIELD_PERCENT\\n',\n",
       " '%\\tI-YIELD_PERCENT\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " '[\\tB-REACTION_PRODUCT\\n',\n",
       " '5-\\tI-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " '2,3-difluorophenyl\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " '-3-methyl-2,4-dioxo-3,4-dihydro-2H-pyrimidin-1-yl\\tI-REACTION_PRODUCT\\n',\n",
       " ']\\tI-REACTION_PRODUCT\\n',\n",
       " '-acetic\\tI-REACTION_PRODUCT\\n',\n",
       " 'acid\\tI-REACTION_PRODUCT\\n',\n",
       " 'is\\tO\\n',\n",
       " 'obtained\\tB-REACTION_STEP\\n',\n",
       " 'in\\tO\\n',\n",
       " 'the\\tO\\n',\n",
       " 'form\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'a\\tO\\n',\n",
       " 'white\\tO\\n',\n",
       " 'solid\\tO\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ner_files([test_path], 'raw_data/test')\n",
    "with io.open('raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-DOCSTART-'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.923 seconds\n",
      "Number of train sentences: 50\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = 'raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = 'raw_data/sample_data'\n",
    "generate_ner_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = 'raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = 'raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = 'raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = 'raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 8.437 seconds\n",
      "Number of train sentences: 900\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = 'data/conll/ee_ner_train'\n",
    "generate_ner_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2.932 seconds\n",
      "Number of train sentences: 225\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = 'data/conll/ee_ner_dev'\n",
    "generate_ner_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 7.688 seconds\n",
      "Number of train sentences: 375\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = 'data/conll/ee_ner_test'\n",
    "generate_ner_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
