{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate inputs for SRE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10782,
     "status": "ok",
     "timestamp": 1637371488548,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zl1fN8rz6CSC"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1637372518523,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [],
   "source": [
    "full_path = '../data/sre_em/sre_em_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_start_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask \n",
    "    that picks out the start marker for each entity \n",
    "    given a list of snippet tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e1_mask[np.argwhere(np.array(snippetTokens) == start1)] = True\n",
    "\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask[np.argwhere(np.array(snippetTokens) == start2)] = True\n",
    "\n",
    "    return e1_mask, e2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_mention_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask\n",
    "    that picks out the tokens for each entity\n",
    "    between (but not including) the entity markers\n",
    "    \"\"\"\n",
    "    \n",
    "    em_markers = [start1, '[/E1]', start2, '[/E2]']\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    in_e1 = False\n",
    "    in_e2 = False\n",
    "    \n",
    "    for (i, t) in enumerate(snippetTokens):\n",
    "        if t in em_markers:\n",
    "            if t in [start1, '[/E1]']:\n",
    "                in_e1 = not in_e1\n",
    "            elif t in [start2, '[/E2]']:\n",
    "                in_e2 = not in_e2\n",
    "        else:\n",
    "            if in_e1 is True:\n",
    "                e1_mask[i] = True\n",
    "            elif in_e2 is True:\n",
    "                e2_mask[i] = True\n",
    "                \n",
    "    return e1_mask, e2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_mention_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask\n",
    "    that picks out the tokens for each entity\n",
    "    between the entity markers, including the ner marker\n",
    "    \"\"\"\n",
    "    \n",
    "    em_markers = [start1, '[/E1]', start2, '[/E2]']\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    in_e1 = False\n",
    "    in_e2 = False\n",
    "    \n",
    "    for (i, t) in enumerate(snippetTokens):\n",
    "        if t in em_markers:\n",
    "            if t in [start1, '[/E1]']:\n",
    "                in_e1 = not in_e1\n",
    "            elif t in [start2, '[/E2]']:\n",
    "                in_e2 = not in_e2\n",
    "        else:\n",
    "            if in_e1 is True:\n",
    "                e1_mask[i] = True\n",
    "            elif in_e2 is True:\n",
    "                e2_mask[i] = True\n",
    "    \n",
    "    x1 = snippetTokens.index(start1)\n",
    "    e1_mask[x1] = True\n",
    "    \n",
    "    x2 = snippetTokens.index(start2)\n",
    "    e2_mask[x2] = True\n",
    "                \n",
    "    return e1_mask, e2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for mask functions\n",
    "tokens = ['[CLS]', 'The', 'Α', 'toxic', 'compound', '[/E1]', 'was', 'Δ', 'heated', 'to', 'reflux', '[/E2]', 'for', 'one', 'hour', '[SEP]']\n",
    "start1 = 'Α'\n",
    "start2 = 'Δ'\n",
    "\n",
    "mask1, mask2 = generate_entity_start_mask(tokens, len(tokens), start1, start2)\n",
    "mask3, mask4 = generate_entity_mention_mask(tokens, len(tokens), start1, start2)\n",
    "mask5, mask6 = generate_ner_mention_mask(tokens, len(tokens), start1, start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xmZ2ypf2AhNy"
   },
   "outputs": [],
   "source": [
    "def generate_input_lists(full_path, marker_type, head_type, max_length=500):\n",
    "    \"\"\"\n",
    "    [Function description here]\n",
    "    \n",
    "    marker_type = marker used: 'em' or 'ner'\n",
    "    head_type = relation representation for classification: 'cls', 'start', 'pool', 'ner'\n",
    "    \"\"\"\n",
    "    \n",
    "    # lists for BERT input\n",
    "    bertTokenIDs = []\n",
    "    bertMasks = []\n",
    "    bertSeqIDs = []\n",
    "    \n",
    "    # list for labels\n",
    "    origLabels = []\n",
    "    codedLabels = []\n",
    "\n",
    "    # lists for entity masks\n",
    "    entity1Masks = []\n",
    "    entity2Masks = []\n",
    "    \n",
    "    # lists for processing\n",
    "    snippetLengthList = []\n",
    "    discardedEntries = []\n",
    "    \n",
    "    # dictionary for converting labels to code\n",
    "    code = {'ARG1': 0, 'ARGM': 1}\n",
    "\n",
    "    # determine which marker list to use\n",
    "    if marker_type == 'em':\n",
    "        markers = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "    elif marker_type == 'ner':\n",
    "        markers = ['Α', 'Β', 'Π', 'Σ', 'Ο', 'Τ', 'Θ', 'Ψ', 'Υ', 'Χ', 'Λ', 'Δ', '[/E1]', '[/E2]']\n",
    "        \n",
    "    \n",
    "    # open file and read lines of text\n",
    "    # each line is an entry\n",
    "    with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    for line in text:\n",
    "\n",
    "        parsed_line = line.strip().split('\\t')\n",
    "\n",
    "        snippet_id = parsed_line[0]\n",
    "        label = parsed_line[1]\n",
    "        snippet = parsed_line[2].split()\n",
    "\n",
    "        # generate inputs for BERT\n",
    "        # convert snippets to tokenIDs, cap snippet length using max_length\n",
    "        # snippets which are shorter than max_length are padded\n",
    "        # snippets which are longer are truncated\n",
    "        # truncated snippets with only one entity are discarded\n",
    "        # all snippets end with a [SEP] token, padded or not\n",
    "\n",
    "        # tokenize snippet, except for entity markers\n",
    "        # identify start markers for each entity\n",
    "        snippetTokens = ['[CLS]']\n",
    "        start1 = ''\n",
    "        start2 = ''\n",
    "        i = 1\n",
    "\n",
    "        for word in snippet:\n",
    "            if word not in markers:\n",
    "                tokens = tokenizer.tokenize(word)\n",
    "                snippetTokens.extend(tokens)\n",
    "            else:\n",
    "                snippetTokens.append(word)\n",
    "                if i == 1:\n",
    "                    start1 = word\n",
    "                if i == 3:\n",
    "                    start2 = word\n",
    "                i += 1\n",
    "\n",
    "        # check that both entities will make it within max_length\n",
    "        # by finding the index for [/E2] and comparing it to (max_length - 1)\n",
    "        check = snippetTokens.index('[/E2]')\n",
    "\n",
    "        # discard if only one entity will make it\n",
    "        if check >= (max_length - 1):\n",
    "            discardedEntries.append(snippet_id)\n",
    "            continue\n",
    "\n",
    "        # create space for at least a final [SEP] token\n",
    "        if len(snippetTokens) >= max_length:\n",
    "            snippetTokens = snippetTokens[:(max_length - 1)]\n",
    "        \n",
    "        # figure out snippet length for padding or truncating\n",
    "        snippetLength = len(snippetTokens) + 1\n",
    "        snippetLengthList.append(snippetLength - 2)\n",
    "\n",
    "        # add [SEP] token and padding\n",
    "        snippetTokens += ['[SEP]'] + ['[PAD]'] * (max_length - snippetLength)\n",
    "\n",
    "        # generate BERT input lists\n",
    "        bertTokenIDs.append(tokenizer.convert_tokens_to_ids(snippetTokens))\n",
    "        bertMasks.append(([1] * snippetLength) + ([0] * (max_length - snippetLength)))\n",
    "        bertSeqIDs.append([0] * (max_length))\n",
    "\n",
    "        # generate label lists\n",
    "        origLabels.append(label)\n",
    "        codedLabels.append(code[label])\n",
    "        \n",
    "        # generate entity masks\n",
    "        if head_type == 'start':\n",
    "            e1_mask, e2_mask = generate_entity_start_mask(snippetTokens, max_length, start1, start2)\n",
    "            entity1Masks.append(e1_mask)\n",
    "            entity2Masks.append(e2_mask)\n",
    "        \n",
    "        elif head_type == 'pool':\n",
    "            e1_mask, e2_mask = generate_entity_mention_mask(snippetTokens, max_length, start1, start2)\n",
    "            entity1Masks.append(e1_mask)\n",
    "            entity2Masks.append(e2_mask)\n",
    "        \n",
    "        elif head_type == 'ner':\n",
    "            e1_mask, e2_mask = generate_ner_mention_mask(snippetTokens, max_length, start1, start2)\n",
    "            entity1Masks.append(e1_mask)\n",
    "            entity2Masks.append(e2_mask)\n",
    "\n",
    "    all_lists = [bertTokenIDs, bertMasks, bertSeqIDs, \n",
    "                 origLabels, codedLabels, \n",
    "                 snippetLengthList, discardedEntries, \n",
    "                 entity1Masks, entity2Masks]\n",
    "    \n",
    "    return all_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data for building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "full_path = '../data/sre_em/sre_em_sample.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample #1: EM, start head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists = generate_input_lists(full_path, marker_type='em', head_type='start', max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [all_lists[0], all_lists[1], all_lists[2], all_lists[7], all_lists[8]]\n",
    "codedLabels = all_lists[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-dd35d951b2e4>:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_test = np.array([testSentence_ids, testMasks, testSequence_ids, testE1Masks, trainE2Masks])\n"
     ]
    }
   ],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "trainE1Masks = []\n",
    "trainE2Masks = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "testE1Masks = []\n",
    "testE2Masks = []\n",
    "\n",
    "labels_train =[]\n",
    "labels_test = []\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        trainE1Masks.append(bert_inputs[3][example])\n",
    "        trainE2Masks.append(bert_inputs[4][example])\n",
    "        labels_train.append(codedLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        testE1Masks.append(bert_inputs[3][example])\n",
    "        testE2Masks.append(bert_inputs[4][example])\n",
    "        labels_test.append(codedLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids, trainMasks, trainSequence_ids, trainE1Masks, trainE2Masks])\n",
    "X_test = np.array([testSentence_ids, testMasks, testSequence_ids, testE1Masks, trainE2Masks])\n",
    "\n",
    "reLabels_train = np.array(labels_train)\n",
    "reLabels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [X_train, reLabels_train]\n",
    "test_all = [X_test, reLabels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../pickles/sample/train_em_start_base_cased.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"../pickles/sample/test_em_start_base_cased.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample #2: EM, pool head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists = generate_input_lists(full_path, marker_type='em', head_type='pool', max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = [all_lists[0], all_lists[1], all_lists[2], all_lists[7], all_lists[8]]\n",
    "codedLabels = all_lists[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-dd35d951b2e4>:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_test = np.array([testSentence_ids, testMasks, testSequence_ids, testE1Masks, trainE2Masks])\n"
     ]
    }
   ],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "trainE1Masks = []\n",
    "trainE2Masks = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "testE1Masks = []\n",
    "testE2Masks = []\n",
    "\n",
    "labels_train =[]\n",
    "labels_test = []\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        trainE1Masks.append(bert_inputs[3][example])\n",
    "        trainE2Masks.append(bert_inputs[4][example])\n",
    "        labels_train.append(codedLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        testE1Masks.append(bert_inputs[3][example])\n",
    "        testE2Masks.append(bert_inputs[4][example])\n",
    "        labels_test.append(codedLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids, trainMasks, trainSequence_ids, trainE1Masks, trainE2Masks])\n",
    "X_test = np.array([testSentence_ids, testMasks, testSequence_ids, testE1Masks, trainE2Masks])\n",
    "\n",
    "reLabels_train = np.array(labels_train)\n",
    "reLabels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [X_train, reLabels_train]\n",
    "test_all = [X_test, reLabels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../pickles/sample/train_em_pool_base_cased.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"../pickles/sample/test_em_pool_base_cased.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbXeVp7ZkpXlI1z13KHxK1",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1ncM7IFEgM8vhjwuZRLUJBL3L9Bh-70pS",
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
