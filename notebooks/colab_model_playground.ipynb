{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QJ_ExX95C8m"
   },
   "source": [
    "#### Set-up for Colab\n",
    "Install dependencies, mount drive, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3553,
     "status": "ok",
     "timestamp": 1637472237730,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "Jo_tjwx24E-H"
   },
   "outputs": [],
   "source": [
    "pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3891,
     "status": "ok",
     "timestamp": 1637472243380,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "a01FnU6G41Jv"
   },
   "outputs": [],
   "source": [
    "pip install -q tf-models-official==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYhBoz05pFKY"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19385,
     "status": "ok",
     "timestamp": 1637467098968,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "KRkiRSVko_DV",
    "outputId": "1ccba214-96c6-4394-e584-a061830fa7cb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1637472718141,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "qbwOc84554BS"
   },
   "outputs": [],
   "source": [
    "#path = 'drive/MyDrive/MIDS/chemical_patent_cer_ee'\n",
    "path = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKnEMa6zo32N"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3608,
     "status": "ok",
     "timestamp": 1637472723438,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/MIDS/chemical_patent_cer_ee/notebooks')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import bert\n",
    "\n",
    "from sre_inputs import *\n",
    "from train_test import *\n",
    "from sre_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzxEMlBo32P"
   },
   "source": [
    "#### BERT Model\n",
    "- Load BERT model and tokenizer\n",
    "- Set max length for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2900,
     "status": "ok",
     "timestamp": 1637472729128,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "7ECV6hEuo32Q",
    "outputId": "26d55830-c22e-4c5f-f19a-971bcfc18da2"
   },
   "outputs": [],
   "source": [
    "# path for bert model\n",
    "bert_model = 'scibert'\n",
    "bert_model_dir = f'{path}/bert/scibert_scivocab_cased'\n",
    "\n",
    "# set tokenizer\n",
    "vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# set BERT model\n",
    "bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
    "bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "# set max length for inputs\n",
    "max_length = 500\n",
    "\n",
    "# set parameters for model type\n",
    "marker_type = 'ner' # 'em', 'ner', or 'std'\n",
    "head_type = 'start' # 'cls', 'start', 'pool', or 'ner'\n",
    "subsampled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Data\n",
    "- Upload preprocessed chemical patent file(s)\n",
    "- Use `sre_inputs` module to generate inputs for model\n",
    "- Sample only: split into train/test using `train_test` module\n",
    "- Need to one hot encode labels before using in model\n",
    "\n",
    "*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1637472741081,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "fBiNc5Vmo32T"
   },
   "outputs": [],
   "source": [
    "#### SAMPLE PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    sample_path = f'{path}/data/sre_em/sre_em_sample.csv'\n",
    "elif marker_type == 'ner':\n",
    "    sample_path = f'{path}/data/sre_ner/sre_ner_sample.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    all_lists = generate_entity_inputs(sample_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    all_lists = generate_standard_inputs(sample_path, tokenizer, max_length)\n",
    "\n",
    "# SAMPLE ONLY: split into train/test\n",
    "train_all, test_all = train_test_split(all_lists)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "sample_inputs_train = [x for x in train_all[0][:5]]\n",
    "sample_labels_train = train_all[1]\n",
    "sample_labels_train = tf.one_hot(sample_labels_train, depth=3)\n",
    "\n",
    "sample_inputs_test = [x for x in test_all[0][:5]]\n",
    "sample_labels_test = test_all[1]\n",
    "sample_labels_test = tf.one_hot(sample_labels_test, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1637472732928,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [],
   "source": [
    "#### TRAIN/DEV DATASET PROCESSING ####\n",
    "\n",
    "# paths for preprocessed data\n",
    "if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n",
    "elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n",
    "elif marker_type == 'ner' and not subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n",
    "elif marker_type == 'ner' and subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n",
    "\n",
    "print(f'Loaded {train_path}')\n",
    "print(f'Loaded {dev_path}')\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n",
    "    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n",
    "    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_train = [x for x in train_lists[0][:5]]\n",
    "train_labels = train_lists[1]\n",
    "model_labels_train = tf.one_hot(train_labels, depth=2)\n",
    "\n",
    "model_inputs_dev = [x for x in dev_lists[0][:5]]\n",
    "dev_labels = dev_lists[1]\n",
    "model_labels_dev = tf.one_hot(dev_labels, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST DATASET PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n",
    "elif marker_type == 'ner':\n",
    "    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_test = [x for x in test_lists[0][:5]]\n",
    "test_labels = test_lists[1]\n",
    "model_labels_test = tf.one_hot(test_labels, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TweKc5oMo32T"
   },
   "source": [
    "#### Run Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 874333,
     "status": "ok",
     "timestamp": 1637473631834,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "3BZ9t2fvo32U",
    "outputId": "52996c72-896f-42f9-de0f-ce1fa459f31d"
   },
   "outputs": [],
   "source": [
    "#### SAMPLE RUN ####\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model = sre_cls_model(bert, max_length)\n",
    "elif head_type == 'start':\n",
    "    model = sre_start_model(bert, max_length)\n",
    "elif head_type == 'pool':\n",
    "    model = sre_pool_model(bert, max_length)\n",
    "elif head_type == 'ner':\n",
    "    model = sre_pool_model(bert, max_length)\n",
    "\n",
    "if marker_type == 'std' or head_type == 'cls':\n",
    "    model.fit(\n",
    "        sample_inputs_train[:3], \n",
    "        {\"sre\": sample_labels_train},\n",
    "        validation_data=(sample_inputs_test[:3], {\"sre\": sample_labels_test}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    model.fit(\n",
    "        sample_inputs_train, \n",
    "        {\"sre\": sample_labels_train},\n",
    "        validation_data=(sample_inputs_test, {\"sre\": sample_labels_test}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAIN/DEV RUN ####\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "if head_type == 'cls':\n",
    "    model = sre_cls_model(bert, max_length)\n",
    "elif head_type == 'start':\n",
    "    model = sre_start_model(bert, max_length)\n",
    "elif head_type == 'pool':\n",
    "    model = sre_pool_model(bert, max_length)\n",
    "elif head_type == 'ner':\n",
    "    model = sre_pool_model(bert, max_length)\n",
    "\n",
    "if marker_type == 'std' or head_type == 'cls':\n",
    "    model.fit(\n",
    "        model_inputs_train[:3], \n",
    "        {\"sre\": model_labels_train},\n",
    "        validation_data=(model_inputs_dev[:3], {\"sre\": model_labels_dev}),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    model.fit(\n",
    "    model_inputs_train, \n",
    "    {\"sre\": model_labels_train},\n",
    "    validation_data=(model_inputs_dev, {\"sre\": model_labels_dev}),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rV0shhro32V",
    "outputId": "25b147ba-a456-4e5a-af1b-525e9d36a587"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: visualize model\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 9149,
     "status": "error",
     "timestamp": 1637473655893,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "xlKiGjCuo32W",
    "outputId": "32f38391-1c97-4b58-f6e8-4640e87e6e2e"
   },
   "outputs": [],
   "source": [
    "# # save model\n",
    "# if subsampled:\n",
    "#     model_name = f'SRE_{bert_model}_{marker_type}_{head_type}_sub'\n",
    "# else:\n",
    "#     model_name = f'SRE_{bert}_{marker_type}_{head_type}'\n",
    "\n",
    "# model.save(f'{path}/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN-Qj8bvCPgA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "colab_sre_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
