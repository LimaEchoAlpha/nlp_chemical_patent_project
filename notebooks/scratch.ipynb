{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QJ_ExX95C8m"
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo_tjwx24E-H"
   },
   "outputs": [],
   "source": [
    "pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a01FnU6G41Jv"
   },
   "outputs": [],
   "source": [
    "pip install -q tf-models-official==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7Ca-ePF8bkq"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10782,
     "status": "ok",
     "timestamp": 1637371488548,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1637370840745,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "qbwOc84554BS"
   },
   "outputs": [],
   "source": [
    "# path = 'drive/MyDrive/MIDS/chemical_patent_cer_ee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "## Generate BERT inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zl1fN8rz6CSC"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1637372518523,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [],
   "source": [
    "# full_path = f'{path}/data/sre_em/sre_em_sample.csv'\n",
    "full_path = '../data/sre_em/sre_em_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1637372465053,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "gs9YnqsVAMcp"
   },
   "outputs": [],
   "source": [
    "max_length = 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1637372525269,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "MZPaKSYrAQ5o"
   },
   "outputs": [],
   "source": [
    "with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bh6TOXlbBTNw"
   },
   "outputs": [],
   "source": [
    "# lists for BERT input\n",
    "bertTokenIDs = []\n",
    "bertMasks = []\n",
    "bertSeqIDs = []\n",
    "\n",
    "# lists for entity masks\n",
    "entity1StartMasks = []\n",
    "entity2StartMasks = []\n",
    "entity1PooledMasks = []\n",
    "entity2PooledMasks = []\n",
    "\n",
    "# list for labels\n",
    "origLabels = []\n",
    "codedLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "xmZ2ypf2AhNy"
   },
   "outputs": [],
   "source": [
    "# list for designating markers\n",
    "em_markers = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "em_start = ['[E1]', '[E2]']\n",
    "em_end = ['[/E1]', '[/E2]']\n",
    "\n",
    "\n",
    "# dictionary for converting labels to code\n",
    "code = {'ARG1': 0, 'ARGM': 1}\n",
    "\n",
    "# lists for processing\n",
    "snippetLengthList = []\n",
    "discardedEntries = []\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    \n",
    "    parsed_line = line.strip().split('\\t')\n",
    "\n",
    "    snippet_id = parsed_line[0]\n",
    "    label = parsed_line[1]\n",
    "    snippet = parsed_line[2].split()\n",
    "\n",
    "    # generate inputs for BERT\n",
    "    # convert snippets to tokenIDs, cap snippet length using max_length\n",
    "    # snippets which are shorter than max_length are padded\n",
    "    # snippets which are longer are truncated\n",
    "    # truncated snippets with only one entity are discarded\n",
    "    # all snippets end with a [SEP] token, padded or not\n",
    "    \n",
    "    # tokenize snippet, except for entity markers\n",
    "    snippetTokens = ['[CLS]']\n",
    "    \n",
    "    for word in snippet:\n",
    "        if word not in em_markers:\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            snippetTokens.extend(tokens)\n",
    "        else:\n",
    "            snippetTokens.append(word)\n",
    "    \n",
    "    # check that both entities will make it within max_length\n",
    "    # by finding the index for [/E2] and comparing it to (max_length - 1)\n",
    "    check = snippetTokens.index('[/E2]')\n",
    "    \n",
    "    # discard if only one entity will make it\n",
    "    if check >= (max_length - 1):\n",
    "        discardedEntries.append(snippet_id)\n",
    "        continue\n",
    "        \n",
    "    # figure out sentence length for padding or truncating\n",
    "    snippetLengthList.append(snippetLength)\n",
    "    \n",
    "    # create space for at least a final [SEP] token\n",
    "    if snippetLength >= max_length - 1:\n",
    "        snippetTokens = snippetTokens[:(max_length - 2)]\n",
    "        \n",
    "    # add [SEP] token and padding\n",
    "    snippetTokens += ['[SEP]'] + ['[PAD]'] * ((max_length -1) - len(snippetTokens))\n",
    "    \n",
    "    # generate BERT input lists\n",
    "    bertTokenIDs.append(tokenizer.convert_tokens_to_ids(snippetTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSeqIDs.append([0] * (max_length))\n",
    "    \n",
    "    # generate label lists\n",
    "    origLabels.append(label)\n",
    "    codedLabels.append(code[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_start_mask(snippetTokens, max_length):\n",
    "    \"\"\"Helper function that generates a mask \n",
    "    that picks out the start marker for each entity \n",
    "    given a list of snippet tokens\"\"\"\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e1_mask[np.argwhere(np.array(snippetTokens) == '[E1]')] = True\n",
    "\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask[np.argwhere(np.array(snippetTokens) == '[E2]')] = True\n",
    "\n",
    "    return e1_mask, e2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_mention_mask(snippetTokens, max_length):\n",
    "    \"\"\"Helper function that generates a mask\n",
    "    that picks out the tokens for each entity\n",
    "    between (but not including) the entity markers\"\"\"\n",
    "    \n",
    "    em_markers = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    in_e1 = False\n",
    "    in_e2 = False\n",
    "    \n",
    "    for (i, t) in enumerate(snippetTokens):\n",
    "        if t in em_markers:\n",
    "            if t in [\"[E1]\", \"[/E1]\"]:\n",
    "                in_e1 = not in_e1\n",
    "            elif t in [\"[E2]\", \"[/E2]\"]:\n",
    "                in_e2 = not in_e2\n",
    "        else:\n",
    "            if in_e1 is True:\n",
    "                e1_mask[i] = True\n",
    "            elif in_e2 is True:\n",
    "                e2_mask[i] = True\n",
    "                \n",
    "    return e1_mask, e2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif marker_type == 'ner':\n",
    "        e1_mask = np.isin(np.array(snippetTokens), ner_start)\n",
    "        e2_mask = np.isin(np.array(snippetTokens), ner_start)\n",
    "\n",
    "\n",
    "ner_start = ['Α', 'Β', 'Π', 'Σ', 'Ο', 'Τ', 'Θ', 'Ψ', 'Υ', 'Χ', 'Λ', 'Δ']\n",
    "ner_markers = ['[/E]', 'Α', 'Β', 'Π', 'Σ', 'Ο', 'Τ', 'Θ', 'Ψ', 'Υ', 'Χ', 'Λ', 'Δ']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbXeVp7ZkpXlI1z13KHxK1",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1ncM7IFEgM8vhjwuZRLUJBL3L9Bh-70pS",
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
