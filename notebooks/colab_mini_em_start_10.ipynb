{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"colab_mini_em_start_10.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3QJ_ExX95C8m"},"source":["#### Set-up for Colab\n","Install dependencies, mount drive, etc."]},{"cell_type":"code","metadata":{"id":"Jo_tjwx24E-H"},"source":["#pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a01FnU6G41Jv"},"source":["#pip install -q tf-models-official==2.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2sQxLaMqQ87"},"source":["pip uninstall tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwLl81u6qQ87","executionInfo":{"elapsed":61075,"status":"ok","timestamp":1637908458927,"user":{"displayName":"Lea Cleary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64","userId":"16569730562812745412"},"user_tz":300},"outputId":"4ffa5c93-0b58-4b31-b6cb-203224adf4bf"},"source":["pip install tensorflow==2.5.0"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow==2.5.0\n","  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n","\u001b[K     |████████████████████████████████| 454.3 MB 18 kB/s \n","\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n","Collecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 44.4 MB/s \n","\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","Collecting grpcio~=1.34.0\n","  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 43.1 MB/s \n","\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n","Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 57.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.7.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n","Collecting flatbuffers~=1.12.0\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n","Collecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68725 sha256=973081f200384c89faf3eede8b3f25b00ad4361c234f4ca359d92a8a39af2803\n","  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n","Successfully built wrapt\n","Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, keras-nightly, flatbuffers, tensorflow\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 3.10.0.2\n","    Uninstalling typing-extensions-3.10.0.2:\n","      Successfully uninstalled typing-extensions-3.10.0.2\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.42.0\n","    Uninstalling grpcio-1.42.0:\n","      Successfully uninstalled grpcio-1.42.0\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.13.3\n","    Uninstalling wrapt-1.13.3:\n","      Successfully uninstalled wrapt-1.13.3\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","Successfully installed flatbuffers-1.12 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 tensorflow-2.5.0 tensorflow-estimator-2.5.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYhBoz05pFKY","executionInfo":{"elapsed":7864,"status":"ok","timestamp":1637908466782,"user":{"displayName":"Lea Cleary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64","userId":"16569730562812745412"},"user_tz":300},"outputId":"ecaf2430-25e0-4b48-e86a-3bfa03dba66e"},"source":["pip install transformers"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 16.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 46.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEkMeci5rW0L","executionInfo":{"elapsed":7425,"status":"ok","timestamp":1637908474194,"user":{"displayName":"Lea Cleary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64","userId":"16569730562812745412"},"user_tz":300},"outputId":"938ac105-d5c6-470a-8155-628d6e18ce25"},"source":["pip install bert-for-tf2"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bert-for-tf2\n","  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n","\u001b[?25l\r\u001b[K     |████████                        | 10 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 135 kB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n","Collecting params-flow>=0.8.0\n","  Downloading params-flow-0.8.2.tar.gz (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.3)\n","Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=c1de66dd7b2185cb221b4aadf918f64aa1353763754f04086f36124e9f2659d6\n","  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=b1e9b644c8ae0587dea0ae0b99ec8c25134e3483bbfe51f308a8806518a8f8cb\n","  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7912 sha256=c1d80eb2e05939b5dd4fdfd0adde3643597549229c6f612cf32bda629837535b\n","  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n","Successfully built bert-for-tf2 params-flow py-params\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRkiRSVko_DV","executionInfo":{"elapsed":30743,"status":"ok","timestamp":1637908504915,"user":{"displayName":"Lea Cleary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64","userId":"16569730562812745412"},"user_tz":300},"outputId":"06b5ef05-a0a6-413d-cbb9-84619ca99c09"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"qbwOc84554BS"},"source":["path = 'drive/MyDrive/MIDS/chemical_patent_cer_ee'\n","# path = '..'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKnEMa6zo32N"},"source":["#### Import Libraries"]},{"cell_type":"code","metadata":{"id":"CckxGheg4_bq"},"source":["import os\n","import io\n","import re\n","import sys\n","import sys\n","sys.path.append('/content/drive/MyDrive/MIDS/chemical_patent_cer_ee/notebooks')\n","\n","import numpy as np\n","import pandas as pd\n","import argparse\n","from time import time\n","import matplotlib.pyplot as plt\n","\n","import pickle\n","from csv import reader\n","\n","import tensorflow as tf\n","from transformers import BertTokenizer\n","import bert\n","\n","from sre_inputs import *\n","from train_test import *\n","from sre_models import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frzxEMlBo32P"},"source":["#### BERT Model\n","- Load BERT model and tokenizer\n","- Set max length for inputs"]},{"cell_type":"code","metadata":{"id":"7ECV6hEuo32Q"},"source":["# path for bert model\n","bert_model_dir = f'{path}/bert/bert_mini'\n","bert_type = bert_model_dir.split('/')[-1]\n","\n","# set tokenizer\n","vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n","tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n","\n","# set BERT model\n","bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n","bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n","\n","# set max length for inputs\n","max_length = 512\n","\n","# set parameters for model type\n","marker_type = 'em' # 'em', 'ner', or 'std'\n","head_type = 'start' # 'cls', 'start', 'pool', or 'ner'\n","subsampled = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSF2-2aJ8m7k"},"source":["#### Data\n","- Upload preprocessed chemical patent file(s)\n","- Use `sre_inputs` module to generate inputs for model\n","- Sample only: split into train/test using `train_test` module\n","- Need to one hot encode labels before using in model\n","\n","*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DeO-o0ko-bjO","outputId":"4edf4a8f-8c24-4adf-a1b0-02764cff6b5e"},"source":["#### TRAIN/DEV DATASET PROCESSING ####\n","\n","# paths for preprocessed data\n","if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n","    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n","    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n","elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n","    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n","    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n","elif marker_type == 'ner' and not subsampled:\n","    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n","    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n","elif marker_type == 'ner' and subsampled:\n","    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n","    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n","\n","print(f'Loaded {train_path}')\n","print(f'Loaded {dev_path}')\n","\n","# generate inputs for model\n","if marker_type == 'em' or marker_type == 'ner':\n","    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n","    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n","elif marker_type == 'std':\n","    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n","    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n","\n","# generate inputs and labels\n","# one hot encode labels\n","model_inputs_train = [x for x in train_lists[0][:5]]\n","train_labels = train_lists[1]\n","model_labels_train = tf.one_hot(train_labels, depth=3)\n","\n","model_inputs_dev = [x for x in dev_lists[0][:5]]\n","dev_labels = dev_lists[1]\n","model_labels_dev = tf.one_hot(dev_labels, depth=3)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded drive/MyDrive/MIDS/chemical_patent_cer_ee/data/sre_em/sre_em_train.csv\n","Loaded drive/MyDrive/MIDS/chemical_patent_cer_ee/data/sre_em/sre_em_dev.csv\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"by_OKfjhrW0R"},"source":["#### TEST DATASET PROCESSING ####\n","\n","# path for preprocessed data\n","if marker_type == 'em' or marker_type == 'std':\n","    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n","elif marker_type == 'ner':\n","    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n","\n","# generate inputs for model\n","if marker_type == 'em' or marker_type == 'ner':\n","    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n","elif marker_type == 'std':\n","    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n","\n","# generate inputs and labels\n","# one hot encode labels\n","model_inputs_test = [x for x in test_lists[0][:5]]\n","test_labels = test_lists[1]\n","model_labels_test = tf.one_hot(test_labels, depth=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TweKc5oMo32T"},"source":["#### Run Model(s)"]},{"cell_type":"code","metadata":{"id":"ZEgXMIZxrW0S"},"source":["#### TRAIN/DEV RUN ####\n","\n","tf.keras.backend.clear_session()\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","epochs = 10\n","batch_size = 32\n","train_layers = 0\n","\n","if head_type == 'cls':\n","    model = sre_cls_model(bert, max_length, train_layers)\n","elif head_type == 'start':\n","    model = sre_start_model(bert, max_length, train_layers)\n","elif head_type == 'pool':\n","    model = sre_pool_model(bert, max_length, train_layers)\n","elif head_type == 'ner':\n","    model = sre_pool_model(bert, max_length, train_layers)\n","\n","if head_type == 'cls':\n","    model.fit(\n","        model_inputs_train[:3], \n","        {\"sre\": model_labels_train},\n","        validation_data=(model_inputs_dev[:3], {\"sre\": model_labels_dev}),\n","        epochs=epochs,\n","        batch_size=batch_size\n","    )\n","else:\n","    model.fit(\n","    model_inputs_train, \n","    {\"sre\": model_labels_train},\n","    validation_data=(model_inputs_dev, {\"sre\": model_labels_dev}),\n","    epochs=epochs,\n","    batch_size=batch_size\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNN7j9XbqQ9D"},"source":["# evaluate model on test data\n","print('Evaluate on test data')\n","\n","if marker_type == 'std' or head_type == 'cls':\n","    results = model.evaluate(model_inputs_test[:3], model_labels_test, batch_size=batch_size)\n","else:\n","    results = model.evaluate(model_inputs_test, model_labels_test, batch_size=batch_size)\n","\n","# generate predictions on new data (probabilities -- the output of the last layer)\n","print(\"Generate predictions for new samples\")\n","\n","if marker_type == 'std' or head_type == 'cls':\n","    predictions = model.predict(model_inputs_test[:3])\n","else: \n","    predictions = model.predict(model_inputs_test)\n","\n","print(\"predictions shape:\", predictions.shape)\n","\n","# save stuff\n","if subsampled:\n","    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}_sub'\n","else:\n","    model_name = f'SRE_{bert_type}_{marker_type}_{head_type}'\n","\n","# save results and predictions\n","outputs = [results, predictions]\n","with open(f'{path}/results/{model_name}.pickle', \"wb\") as f:\n","    pickle.dump(outputs, f)\n","\n","# save model\n","# model.save(f'{path}/models/{model_name}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cN4L0zFqQ9D"},"source":["# # how to open saved file\n","# with open(f'{path}/results/{model_name}.pickle', \"rb\") as f:\n","#     saved_outputs = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rV0shhro32V"},"source":["# OPTIONAL: visualize model\n","#tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KN-Qj8bvCPgA"},"source":[""],"execution_count":null,"outputs":[]}]}