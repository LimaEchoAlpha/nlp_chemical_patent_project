{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKnEMa6zo32N"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CckxGheg4_bq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import bert\n",
    "\n",
    "from sre_inputs import *\n",
    "from train_test import *\n",
    "from sre_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzxEMlBo32P"
   },
   "source": [
    "#### BERT Model\n",
    "- Load BERT model and tokenizer\n",
    "- Set max length for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ECV6hEuo32Q"
   },
   "outputs": [],
   "source": [
    "path = '..'\n",
    "\n",
    "# path for bert model\n",
    "bert_model_dir = f'{path}/bert/bert_mini'\n",
    "bert_type = bert_model_dir.split('/')[-1]\n",
    "\n",
    "# set tokenizer\n",
    "vocab_file = os.path.join(bert_model_dir, \"vocab.txt\")\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# set BERT model\n",
    "bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
    "bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "# set max length for inputs\n",
    "max_length = 512\n",
    "\n",
    "# set parameters for model type\n",
    "marker_type = 'std' # 'em', 'ner', or 'std'\n",
    "head_type = 'cls' # 'cls', 'start', 'pool', or 'ner'\n",
    "subsampled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSF2-2aJ8m7k"
   },
   "source": [
    "#### Data\n",
    "- Upload preprocessed chemical patent file(s)\n",
    "- Use `sre_inputs` module to generate inputs for model\n",
    "- Sample only: split into train/test using `train_test` module\n",
    "- Need to one hot encode labels before using in model\n",
    "\n",
    "*NB: Make sure that preprocessed data being uploaded and parameters chosen for generating inputs **both** match the type of model it will be used for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fBiNc5Vmo32T"
   },
   "outputs": [],
   "source": [
    "#### SAMPLE PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    sample_path = f'{path}/data/sre_em/sre_em_sample.csv'\n",
    "elif marker_type == 'ner':\n",
    "    sample_path = f'{path}/data/sre_ner/sre_ner_sample.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    all_lists = generate_entity_inputs(sample_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    all_lists = generate_standard_inputs(sample_path, tokenizer, max_length)\n",
    "\n",
    "# SAMPLE ONLY: split into train/test\n",
    "train_all, test_all = train_test_split(all_lists)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "sample_inputs_train = [x for x in train_all[0][:5]]\n",
    "sample_labels_train = train_all[1]\n",
    "sample_labels_train = tf.one_hot(sample_labels_train, depth=3)\n",
    "\n",
    "sample_inputs_test = [x for x in test_all[0][:5]]\n",
    "sample_labels_test = test_all[1]\n",
    "sample_labels_test = tf.one_hot(sample_labels_test, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DeO-o0ko-bjO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/sre_em/sre_em_train.csv\n",
      "Loaded ../data/sre_em/sre_em_dev.csv\n"
     ]
    }
   ],
   "source": [
    "#### TRAIN/DEV DATASET PROCESSING ####\n",
    "\n",
    "# paths for preprocessed data\n",
    "if (marker_type == 'em' or marker_type == 'std') and not subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev.csv'\n",
    "elif (marker_type == 'em' or marker_type == 'std') and subsampled:\n",
    "    train_path = f'{path}/data/sre_em/sre_em_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_em/sre_em_dev_subsampled.csv'\n",
    "elif marker_type == 'ner' and not subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev.csv'\n",
    "elif marker_type == 'ner' and subsampled:\n",
    "    train_path = f'{path}/data/sre_ner/sre_ner_train_subsampled.csv'\n",
    "    dev_path = f'{path}/data/sre_ner/sre_ner_dev_subsampled.csv'\n",
    "\n",
    "print(f'Loaded {train_path}')\n",
    "print(f'Loaded {dev_path}')\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    train_lists = generate_entity_inputs(train_path, tokenizer, marker_type, head_type, max_length)\n",
    "    dev_lists = generate_entity_inputs(dev_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    train_lists = generate_standard_inputs(train_path, tokenizer, max_length)\n",
    "    dev_lists = generate_standard_inputs(dev_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_train = [x for x in train_lists[0][:5]]\n",
    "train_labels = train_lists[1]\n",
    "model_labels_train = tf.one_hot(train_labels, depth=3)\n",
    "\n",
    "model_inputs_dev = [x for x in dev_lists[0][:5]]\n",
    "dev_labels = dev_lists[1]\n",
    "model_labels_dev = tf.one_hot(dev_labels, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "by_OKfjhrW0R"
   },
   "outputs": [],
   "source": [
    "#### TEST DATASET PROCESSING ####\n",
    "\n",
    "# path for preprocessed data\n",
    "if marker_type == 'em' or marker_type == 'std':\n",
    "    test_path = f'{path}/data/sre_em/sre_em_test.csv'\n",
    "elif marker_type == 'ner':\n",
    "    test_path = f'{path}/data/sre_ner/sre_ner_test.csv'\n",
    "\n",
    "# generate inputs for model\n",
    "if marker_type == 'em' or marker_type == 'ner':\n",
    "    test_lists = generate_entity_inputs(test_path, tokenizer, marker_type, head_type, max_length)\n",
    "elif marker_type == 'std':\n",
    "    test_lists = generate_standard_inputs(test_path, tokenizer, max_length)\n",
    "\n",
    "# generate inputs and labels\n",
    "# one hot encode labels\n",
    "model_inputs_test = [x for x in test_lists[0][:5]]\n",
    "test_labels = test_lists[1]\n",
    "model_labels_test = tf.one_hot(test_labels, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN-Qj8bvCPgA"
   },
   "source": [
    "#### Train Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 45805\n",
      "Dev: 10673\n",
      "Test: 18488\n"
     ]
    }
   ],
   "source": [
    "# total number of snippets\n",
    "print(f'Train: {len(model_inputs_train[0])}')\n",
    "print(f'Dev: {len(model_inputs_dev[0])}')\n",
    "print(f'Test: {len(model_inputs_test[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values :  [0 1 2]\n",
      "Occurrence Count :  [31516  9692  4597]\n"
     ]
    }
   ],
   "source": [
    "# train label break down\n",
    "uniqueValues, occurCount = np.unique(train_labels, return_counts=True)\n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values :  [0 1 2]\n",
      "Occurrence Count :  [7341 2247 1085]\n"
     ]
    }
   ],
   "source": [
    "# dev label break down\n",
    "uniqueValues, occurCount = np.unique(dev_labels, return_counts=True)\n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values :  [0 1 2]\n",
      "Occurrence Count :  [12712  3896  1880]\n"
     ]
    }
   ],
   "source": [
    "# test label break down\n",
    "uniqueValues, occurCount = np.unique(test_labels, return_counts=True)\n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 287\n",
      "Min: 6\n",
      "Mean: 58.02\n",
      "Median: 51.0\n"
     ]
    }
   ],
   "source": [
    "# snippet length EDA\n",
    "print(f'Max: {max(train_lists[2][1])}')\n",
    "print(f'Min: {min(train_lists[2][1])}')\n",
    "print(f'Mean: {np.mean(train_lists[2][1]):.2f}')\n",
    "print(f'Median: {np.median(train_lists[2][1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 287\n",
      "Min: 6\n",
      "Mean: 57.47\n",
      "Median: 50.0\n"
     ]
    }
   ],
   "source": [
    "# snippet length EDA\n",
    "print(f'Max: {max(dev_lists[2][1])}')\n",
    "print(f'Min: {min(dev_lists[2][1])}')\n",
    "print(f'Mean: {np.mean(dev_lists[2][1]):.2f}')\n",
    "print(f'Median: {np.median(dev_lists[2][1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 245\n",
      "Min: 6\n",
      "Mean: 57.87\n",
      "Median: 51.0\n"
     ]
    }
   ],
   "source": [
    "# snippet length EDA\n",
    "print(f'Max: {max(test_lists[2][1])}')\n",
    "print(f'Min: {min(test_lists[2][1])}')\n",
    "print(f'Mean: {np.mean(test_lists[2][1]):.2f}')\n",
    "print(f'Median: {np.median(test_lists[2][1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lists[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "colab_sre_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
