{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       and labels it with BIO tags based on cleaned offsets \n",
    "       from its .ann file\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_sentence = patent sentence with annotations for each entity\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    entity_id = 'X'\n",
    "    \n",
    "    sentence = [[x.strip(punctuation), label, entity_id] \n",
    "                for x in full_text[:end].replace('\\n', ' ').split(' ')\n",
    "                if x.strip(punctuation)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        entity_id = offsets[i][2]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            \n",
    "            if label == 'O':            \n",
    "                terms = [[x.strip(punctuation), label, entity_id] \n",
    "                         for x in full_text[start:end].replace('\\n', ' ').split(' ') \n",
    "                         if x.strip(punctuation)]\n",
    "                sentence.extend(terms)\n",
    "            \n",
    "            else:\n",
    "                terms = [x \n",
    "                         for x in full_text[start:end].replace('\\n', ' ').split(' ') \n",
    "                         if x]\n",
    "                bio_terms = [[terms[0], 'B-' + label, entity_id]]\n",
    "                if len(terms) > 1:\n",
    "                    bio_terms.extend([[terms[i], 'I-' + label, entity_id] for i in range(1, len(terms))])\n",
    "                sentence.extend(bio_terms)\n",
    "        \n",
    "        else:\n",
    "            terms = [[x.strip(punctuation), label, entity_id] \n",
    "                     for x in full_text[start:].replace('\\n', ' ').split(' ') \n",
    "                     if x.strip(punctuation)]  \n",
    "            sentence.extend(terms)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file for use in NER task\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    sentences = []\n",
    "    corrections = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        sentences.append([f'SENTENCE: {file[-4:]}'])\n",
    "        corrections.append([f'SENTENCE: {file[-4:]}'])\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        corrections.append(file_corrections)\n",
    "        \n",
    "        ann_sentence = process_txt(f'{file}.txt', cleaned_offsets)\n",
    "        sentences.extend(ann_sentence)\n",
    "        \n",
    "        #sentences.append([f'COUNT: {len(ann_sentence)}'])\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(sentences)\n",
    "    \n",
    "    with open(f'{output_path}_corrections.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(corrections)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'raw_data/EE/ee_train/1029'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 5: Synthesis of racemic 3-aminoquinuclidine.mono-p-toluenesulfonate\\n2-propanol solution (12.12 g) containing racemic 3-aminoquinuclidine (3.02 g: 23.9 mmol) was charged into a 50 mL-flask equipped with a mechanical stirrer, a thermometer, and a cooling line, and 2-propanol solution (19.48 g) containing p-toluenesulfonic acid monohydrate (4.53 g: 23.8 mmol) was added thereto while stirring at room temperature. Precipitated crystals were filtered, and then dried under reduced pressure at 60° C., to obtain racemic 3-aminoquinuclidine.mono-p-toluenesulfonate (6.76 g: 22.7 mmol, yield=95%) as a white crystal.\\nEndothermic peak top temperature in DSC: 198° C.\\nElemental analysis: C7H14N2.C7H8O3S\\nTheoretical value: C, 56.35%, H, 7.43%, N, 9.39%, S 10.74%\\nActual measured value: C, 56.4%, H, 7.5%, N, 9.3%, S 10.2%'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'EXAMPLE_LABEL 8 9', '5'],\n",
       " ['T1', 'STARTING_MATERIAL 312 346', 'p-toluenesulfonic acid monohydrate'],\n",
       " ['T2',\n",
       "  'REACTION_PRODUCT 517 568',\n",
       "  'racemic 3-aminoquinuclidine.mono-p-toluenesulfonate'],\n",
       " ['T3', 'TEMPERATURE 499 504', '60° C'],\n",
       " ['T12', 'REACTION_STEP 169 176', 'charged']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'T']\n",
    "ann[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_corrections = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'EXAMPLE_LABEL', 'T0'),\n",
       " (9, 'O', 'X'),\n",
       " (24, 'REACTION_PRODUCT', 'T10'),\n",
       " (75, 'O', 'X'),\n",
       " (76, 'SOLVENT', 'T7')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = process_txt(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Example', 'O', 'X'],\n",
       " ['5', 'B-EXAMPLE_LABEL', 'T0'],\n",
       " ['Synthesis', 'O', 'X'],\n",
       " ['of', 'O', 'X'],\n",
       " ['racemic', 'B-REACTION_PRODUCT', 'T10']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.087 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SENTENCE: 1029\\n',\n",
       " 'Example\\tO\\tX\\n',\n",
       " '5\\tB-EXAMPLE_LABEL\\tT0\\n',\n",
       " 'Synthesis\\tO\\tX\\n',\n",
       " 'of\\tO\\tX\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ner_files([test_path], 'raw_data/test')\n",
    "with io.open('raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = 'raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = 'raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = 'raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = 'raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 7.175 seconds\n",
      "Number of train sentences: 900\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "# print(len(filepath_train))\n",
    "# filepath_train[:5]\n",
    "\n",
    "output_train = 'data/ee_ner_train'\n",
    "generate_ner_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5.112 seconds\n",
      "Number of train sentences: 225\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = 'data/ee_ner_dev'\n",
    "generate_ner_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 8.986 seconds\n",
      "Number of train sentences: 375\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = 'data/ee_ner_test'\n",
    "generate_ner_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:9] == 'SENTENCE:']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
