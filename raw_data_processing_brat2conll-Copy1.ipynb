{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for labeling corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       corrections = dictionary of entity ID mappings for overlapping offsets\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        ann = [x.strip().split('\\t') for x in text.readlines() if x.strip().split('\\t')[0][0] == 'T']\n",
    "    \n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    return cleaned_offsets, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_labeler(chunks):\n",
    "    \"\"\"Helper function that further processes annotated chunks from ann_chunker()\n",
    "       Tokenizes the chunks and applies BIO labels to each token\n",
    "       \n",
    "       Inputs:\n",
    "       chunks = list of annotated chunks based on .ann file offsets\n",
    "                format: (chunk, label)\n",
    "       \n",
    "       Output:\n",
    "       bio_doc = document transformed into a list of tokens with bio labels\"\"\"\n",
    "    \n",
    "    bio_tokens = []\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label = tup\n",
    "        \n",
    "        if label == 'O':\n",
    "            if chunk[:1] == '\\n':\n",
    "                bio_tokens.append([])\n",
    "            sentences = sent_tokenize(chunk.strip())\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        bio_tokens.append([x, label])\n",
    "                        if x == '.':\n",
    "                            bio_tokens.append([])\n",
    "                \n",
    "        else:\n",
    "            tokens = [x for x in word_tokenize(chunk)]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    bio_tokens.append([tokens[i], 'B-' + label])\n",
    "                else:\n",
    "                    bio_tokens.append([tokens[i], 'I-' + label])\n",
    "    \n",
    "    return bio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file in CoNLL 2003 format (for use in NER task)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    docs = []\n",
    "    corrections = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        docs.append(['-DOCSTART- -X- O O'])\n",
    "        corrections.append([f'SENTENCE: {file[-4:]}'])\n",
    "        \n",
    "        cleaned_offsets, file_corrections = process_ann(f'{file}.ann')\n",
    "        corrections.append(file_corrections)\n",
    "        \n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        bio_tokens = bio_labeler(chunks)\n",
    "        docs.extend(bio_tokens)\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(docs)\n",
    "    \n",
    "    with open(f'{output_path}_corrections.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(corrections)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'raw_data/sample_ee/0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 194\\n3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione (racemate)\\n813 mg (1.84 mmol) of the compound from Example 243A were dissolved in 40 ml of dioxane, and 461 mg (2.76 mmol) of CDI were added. The mixture was stirred at RT for 16 h. The reaction solution was then concentrated on a rotary evaporator. The residue was dissolved in 15 ml of DMSO and this solution was purified by means of preparative HPLC (Method 14). Combination of the product fractions and freeze-drying gave 383 mg (42% of theory) of the title compound'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'OTHER_COMPOUND 417 421', 'DMSO'],\n",
       " ['T1', 'TIME 305 309', '16 h'],\n",
       " ['T2', 'REACTION_PRODUCT 585 599', 'title compound']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'T']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_corrections = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'EXAMPLE_LABEL', 'T8'), (11, 'O', 'X'), (12, 'REACTION_PRODUCT', 'T6')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Example ', 'O'), ('194', 'EXAMPLE_LABEL'), ('\\n', 'O')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Example', 'O'], ['194', 'B-EXAMPLE_LABEL'], []]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_doc = bio_labeler(trial_sentence)\n",
    "trial_doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trial_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.052 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['-DOCSTART- -X- O O\\n',\n",
       " 'Example\\tO\\n',\n",
       " '194\\tB-EXAMPLE_LABEL\\n',\n",
       " '\\n',\n",
       " '3-Isobutyl-5-methyl-1-\\tB-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " 'oxetan-2-ylmethyl\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " '-6-\\tI-REACTION_PRODUCT\\n',\n",
       " '[\\tI-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " '2-oxoimidazolidin-1-yl\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " 'methyl\\tI-REACTION_PRODUCT\\n',\n",
       " ']\\tI-REACTION_PRODUCT\\n',\n",
       " 'thieno\\tI-REACTION_PRODUCT\\n',\n",
       " '[\\tI-REACTION_PRODUCT\\n',\n",
       " '2,3-d\\tI-REACTION_PRODUCT\\n',\n",
       " ']\\tI-REACTION_PRODUCT\\n',\n",
       " 'pyrimidine-2,4\\tI-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " '1H,3H\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " '-dione\\tI-REACTION_PRODUCT\\n',\n",
       " '(\\tI-REACTION_PRODUCT\\n',\n",
       " 'racemate\\tI-REACTION_PRODUCT\\n',\n",
       " ')\\tI-REACTION_PRODUCT\\n',\n",
       " '\\n',\n",
       " '813\\tO\\n',\n",
       " 'mg\\tO\\n',\n",
       " '(\\tO\\n',\n",
       " '1.84\\tO\\n',\n",
       " 'mmol\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'the\\tO\\n',\n",
       " 'compound\\tB-STARTING_MATERIAL\\n',\n",
       " 'from\\tI-STARTING_MATERIAL\\n',\n",
       " 'Example\\tI-STARTING_MATERIAL\\n',\n",
       " '243A\\tI-STARTING_MATERIAL\\n',\n",
       " 'were\\tO\\n',\n",
       " 'dissolved\\tB-REACTION_STEP\\n',\n",
       " 'in\\tO\\n',\n",
       " '40\\tO\\n',\n",
       " 'ml\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'dioxane\\tB-SOLVENT\\n',\n",
       " ',\\tO\\n',\n",
       " 'and\\tO\\n',\n",
       " '461\\tO\\n',\n",
       " 'mg\\tO\\n',\n",
       " '(\\tO\\n',\n",
       " '2.76\\tO\\n',\n",
       " 'mmol\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'CDI\\tB-STARTING_MATERIAL\\n',\n",
       " 'were\\tO\\n',\n",
       " 'added\\tB-REACTION_STEP\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'mixture\\tO\\n',\n",
       " 'was\\tO\\n',\n",
       " 'stirred\\tB-REACTION_STEP\\n',\n",
       " 'at\\tO\\n',\n",
       " 'RT\\tB-TEMPERATURE\\n',\n",
       " 'for\\tO\\n',\n",
       " '16\\tB-TIME\\n',\n",
       " 'h\\tI-TIME\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'reaction\\tO\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'was\\tO\\n',\n",
       " 'then\\tO\\n',\n",
       " 'concentrated\\tO\\n',\n",
       " 'on\\tO\\n',\n",
       " 'a\\tO\\n',\n",
       " 'rotary\\tO\\n',\n",
       " 'evaporator\\tO\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'The\\tO\\n',\n",
       " 'residue\\tO\\n',\n",
       " 'was\\tO\\n',\n",
       " 'dissolved\\tB-WORKUP\\n',\n",
       " 'in\\tO\\n',\n",
       " '15\\tO\\n',\n",
       " 'ml\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'DMSO\\tB-OTHER_COMPOUND\\n',\n",
       " 'and\\tO\\n',\n",
       " 'this\\tO\\n',\n",
       " 'solution\\tO\\n',\n",
       " 'was\\tO\\n',\n",
       " 'purified\\tO\\n',\n",
       " 'by\\tO\\n',\n",
       " 'means\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'preparative\\tO\\n',\n",
       " 'HPLC\\tO\\n',\n",
       " '(\\tO\\n',\n",
       " 'Method\\tO\\n',\n",
       " '14\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " '.\\tO\\n',\n",
       " '\\n',\n",
       " 'Combination\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'the\\tO\\n',\n",
       " 'product\\tO\\n',\n",
       " 'fractions\\tO\\n',\n",
       " 'and\\tO\\n',\n",
       " 'freeze-drying\\tO\\n',\n",
       " 'gave\\tB-REACTION_STEP\\n',\n",
       " '383\\tB-YIELD_OTHER\\n',\n",
       " 'mg\\tI-YIELD_OTHER\\n',\n",
       " '(\\tO\\n',\n",
       " '42\\tB-YIELD_PERCENT\\n',\n",
       " '%\\tI-YIELD_PERCENT\\n',\n",
       " 'of\\tO\\n',\n",
       " 'theory\\tO\\n',\n",
       " ')\\tO\\n',\n",
       " 'of\\tO\\n',\n",
       " 'the\\tO\\n',\n",
       " 'title\\tB-REACTION_PRODUCT\\n',\n",
       " 'compound\\tI-REACTION_PRODUCT\\n']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ner_files([test_path], 'raw_data/test')\n",
    "with io.open('raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-DOCSTART-'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.923 seconds\n",
      "Number of train sentences: 50\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = 'raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = 'raw_data/sample_data'\n",
    "generate_ner_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = 'raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = 'raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = 'raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = 'raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 8.437 seconds\n",
      "Number of train sentences: 900\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = 'data/conll/ee_ner_train'\n",
    "generate_ner_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2.932 seconds\n",
      "Number of train sentences: 225\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = 'data/conll/ee_ner_dev'\n",
    "generate_ner_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 7.688 seconds\n",
      "Number of train sentences: 375\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = 'data/conll/ee_ner_test'\n",
    "generate_ner_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:10] == '-DOCSTART-']\n",
    "print(f'Number of train sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
