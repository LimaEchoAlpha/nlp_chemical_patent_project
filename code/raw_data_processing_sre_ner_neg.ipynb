{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from string import punctuation\n",
    "from time import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file, test=False):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for identifying entities and relations \n",
    "       in corresponding .txt file.\n",
    "       Also adds negative relations within the span of +/- 5 entities \n",
    "       from each trigger word.\n",
    "       If generating test data, only includes positive relations within\n",
    "       the span of +/- 5 entities from the trigger word.\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       span = number of entities from the trigger word to look for pairs\n",
    "       test = whether or not test data is being generated\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       relations = list of tuples for extracting relations from corresponding .txt file\n",
    "                   format: (relation ID, relation_type, entity ID #1, entity ID #2)\n",
    "       positives_missed = count of positive relations missed\"\"\"\n",
    "    \n",
    "    # dictionary to convert NER labels to NER-specific markers \n",
    "    # for BERT input coded with seldom used tokens\n",
    "    ner_markers = {'STARTING_MATERIAL': 'Α', 'REAGENT_CATALYST': 'Β', 'REACTION_PRODUCT': 'Π', \n",
    "                   'SOLVENT': 'Σ', 'OTHER_COMPOUND': 'Ο', 'EXAMPLE_LABEL': 'Χ', \n",
    "                   'TIME': 'Τ', 'TEMPERATURE': 'Θ', 'YIELD_PERCENT': 'Ψ', 'YIELD_OTHER': 'Υ',\n",
    "                   'WORKUP': 'Λ', 'REACTION_STEP': 'Δ'}\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = [x.strip().split('\\t') for x in f.readlines()]\n",
    "        \n",
    "    ann = [x for x in text if x[0][0] == 'T']\n",
    "    rel = [x for x in text if x[0][0] == 'R']\n",
    "    \n",
    "    # extract information for identifying entities\n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', ner_markers[label], entity_id))\n",
    "        offsets.append((end, 'E', ner_markers[label], entity_id))\n",
    "    \n",
    "    # sort offsets and clean overlapping entries\n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                if tup[2] != hold[1]:\n",
    "                    corrections.update({tup[3]:'X'})\n",
    "                else:\n",
    "                    corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    # extract information for identifying relations\n",
    "    relations = []\n",
    "    positives = {}\n",
    "    included = []\n",
    "    \n",
    "    # add positive relations\n",
    "    for r in rel:\n",
    "        relation_id = r[0]\n",
    "        relation_type = r[1].split()[0]\n",
    "        entity1 = r[1].split()[1][5:]\n",
    "        entity2 = r[1].split()[2][5:]\n",
    "        \n",
    "        if entity1 in corrections.keys():\n",
    "            if corrections[entity1] == 'X':\n",
    "                continue\n",
    "            else:\n",
    "                entity1 = corrections[entity1]\n",
    "        if entity2 in corrections.keys():\n",
    "            if corrections[entity2] == 'X':\n",
    "                continue\n",
    "            else:\n",
    "                entity2 = corrections[entity2]\n",
    "        \n",
    "        positives.update({(entity1, entity2):(relation_id, relation_type)})\n",
    "        \n",
    "        if not test:\n",
    "            relations.append((relation_id, relation_type, entity1, entity2))\n",
    "            included.append((entity1, entity2))\n",
    "    \n",
    "    # negative relations\n",
    "    negatives = []\n",
    "    triggers = {x[2] for x in cleaned_offsets \n",
    "                if (x[1] == ner_markers['WORKUP'] or x[1] == ner_markers['REACTION_STEP'])}\n",
    "    entity_order = [x[2] for x in cleaned_offsets if (x[2] != 'X' and x[1] != ner_markers['EXAMPLE_LABEL'])]\n",
    "    trigger_indices = {i for i in range(len(entity_order)) if entity_order[i] in triggers}\n",
    "    \n",
    "    # find negative relations\n",
    "    for index in trigger_indices:\n",
    "        \n",
    "        # find indices in span of +/- 5 from trigger\n",
    "        find_span = [index - (i+1) for i in range(5)] + [index + (i+1) for i in range(5)]\n",
    "        real_span = [i for i in find_span if (i >= 0 and i < len(entity_order))]\n",
    "        final_span = [i for i in real_span if i not in trigger_indices]\n",
    "        \n",
    "        # make tuples of trigger words and entities from span indices\n",
    "        potential_pairs = [(entity_order[index], entity_order[i]) for i in final_span]\n",
    "        \n",
    "        # check if tuple is in positives\n",
    "        # only add to negatives if not in positives\n",
    "        for pair in potential_pairs:\n",
    "            if pair in positives.keys():\n",
    "                if not test:\n",
    "                    continue\n",
    "                elif test:\n",
    "                    if pair not in included:\n",
    "                        relations.append((positives[pair][0], positives[pair][1], pair[0], pair[1]))\n",
    "                        included.append(pair)\n",
    "            else:\n",
    "                negatives.append(pair)\n",
    "    \n",
    "    # make a list of negative relations \n",
    "    # in same format as positives to add to relations list\n",
    "    i = 0\n",
    "    for pair in negatives:\n",
    "        negative_id = f'N{i}'\n",
    "        relations.append((negative_id, 'NONE', pair[0], pair[1]))\n",
    "        i += 1\n",
    "        \n",
    "    # count how many positive relations were missed\n",
    "    positives_missed = len(positives.keys()) - len(included) \n",
    "    \n",
    "    return cleaned_offsets, relations, positives_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label, entity ID)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    entity_id = 'X'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label, entity_id)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        entity_id = offsets[i][2]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label, entity_id)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label, entity_id)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_input(snippet_id, rel_tup, chunks):\n",
    "    \"\"\"Helper function that creates one input snippet for BERT SRE:\n",
    "       Inserts entity markers and truncates snippet to include only\n",
    "       sentences containing the entities\n",
    "    \n",
    "       Inputs:\n",
    "       snippet_id = filename of snippet\n",
    "       rel_tup = tuple from relations list generated by process_ann()\n",
    "             format: (relation ID, relation_type, entity ID #1, entity ID #2)\n",
    "       chunks = list of annotated chunks from ann_chunker()\n",
    "    \n",
    "       Output:\n",
    "       rel_input = input snippet ready for BERT SRE\n",
    "                   format: [snippet_id+relation_id]/t[relation_type]/t[cleaned snippet with ner markers]\"\"\"\n",
    "    \n",
    "    # unpack relation_tup\n",
    "    relation_id = rel_tup[0]\n",
    "    relation_type = rel_tup[1]\n",
    "    entity_list = [rel_tup[2], rel_tup[3]]\n",
    "    \n",
    "    new_id = snippet_id + '-' + relation_id\n",
    "    \n",
    "    # build cleaned snippet with ner markers\n",
    "    snippets = []\n",
    "    i = 1\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label, entity = tup\n",
    "        \n",
    "        # clean chunk: remove punctuation,\n",
    "        # word tokenize if not an entity\n",
    "        # split by whitespace if entity\n",
    "        processed_chunk = []\n",
    "            \n",
    "        if label == 'O':\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk.strip())\n",
    "            sentences = sent_tokenize(nopunct)\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        processed_chunk.append(x)\n",
    "                \n",
    "        else:\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk)\n",
    "            tokens = [x for x in nopunct.split(' ') if x]\n",
    "            for t in tokens:\n",
    "                processed_chunk.append(t)\n",
    "        \n",
    "        # add ner markers before and after entities in relation\n",
    "        if entity in entity_list:\n",
    "            snippets.append(label)\n",
    "            if i == 1:\n",
    "                e1 = label\n",
    "            snippets.extend(processed_chunk)\n",
    "            snippets.append(f'[/E{i}]')\n",
    "            i += 1\n",
    "        \n",
    "        else:\n",
    "            snippets.extend(processed_chunk)\n",
    "\n",
    "    # keep only sentences containing the entities\n",
    "    # sentences are marked by periods\n",
    "    e1_index = snippets.index(e1)\n",
    "    e2_index = snippets.index('[/E2]')\n",
    "    \n",
    "    periods = [i for i in range(len(snippets)) if snippets[i] == '.']\n",
    "    periods_before = [i for i in periods if i < e1_index]\n",
    "    periods_after = [i for i in periods if i > e2_index]\n",
    "    \n",
    "    if periods_before:\n",
    "        start = max(periods_before) + 1\n",
    "    else:\n",
    "        start = 0\n",
    "    \n",
    "    if periods_after:\n",
    "        end = min(periods_after) + 1\n",
    "    else:\n",
    "        end = None\n",
    "    \n",
    "    truncated_snippet = snippets[start:end]\n",
    "    \n",
    "    # join snippet chunks to one clean snippet\n",
    "    cleaned_snippet = ' '.join(truncated_snippet)\n",
    "    \n",
    "    return [new_id, relation_type, cleaned_snippet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_re_files(filepaths, output_path, test=False):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file with snippets ready for BERT SRE (one snippet per line)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\n",
    "       missed_count = count of total positive relations missed\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    snippets = []\n",
    "    missed_positives = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        snippet_id = file[-4:]\n",
    "        \n",
    "        cleaned_offsets, relations, missed = process_ann(f'{file}.ann', test=test)\n",
    "        missed_positives.append(missed)\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        \n",
    "        for tup in relations:\n",
    "            line = relation_input(snippet_id, tup, chunks)\n",
    "            snippets.append(line)\n",
    "    \n",
    "    missed_count = sum(missed_positives)\n",
    "    print(f'Number of positive relations missed: {missed_count}')\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(snippets)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_re_files_subsampled(filepaths, output_path, fraction=0.5):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file with snippets ready for BERT SRE (one snippet per line).\n",
    "       Subsampled 25% of negative relations randomly.\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    snippets = []\n",
    "    missed_positives = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        snippet_id = file[-4:]\n",
    "        \n",
    "        cleaned_offsets, relations, missed = process_ann(f'{file}.ann')\n",
    "        missed_positives.append(missed)\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        \n",
    "        # add all positive relations\n",
    "        positive_relations = [tup for tup in relations if tup[0][0] == 'R']\n",
    "        \n",
    "        for tup in positive_relations:\n",
    "            line = relation_input(snippet_id, tup, chunks)\n",
    "            snippets.append(line)\n",
    "        \n",
    "        # keep only 25% of negative relations\n",
    "        negative_relations = [tup for tup in relations if tup[0][0] == 'N']\n",
    "        neg_count = len(negative_relations)\n",
    "        np.random.seed(424)\n",
    "        negative_keep = np.random.binomial(1, fraction, neg_count)\n",
    "        \n",
    "        for i in range(neg_count):\n",
    "            if negative_keep[i] == 1:\n",
    "                line = relation_input(snippet_id, negative_relations[i], chunks)\n",
    "                snippets.append(line)\n",
    "                \n",
    "    missed_count = sum(missed_positives)\n",
    "    print(f'Number of positive relations missed: {missed_count}')\n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(snippets)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../raw_data/EE/ee_train/1069'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Intermediate 2: 2,3,4,5,6,7-hexahydropyrrolo[3',4':3,4]pyrazolo[1 ,5-b][1,2]thiazine-1,1-dioxide p-toluenesulfonate\\nStep 1: tert-butyl 3-iodo-4,6-dihydropyrrolo[3,4-c]pyrazole-5(2H)-carboxylate\\nTo a solution of tert-butyl 4,6-dihydropyrrolo[3,4-c]pyrazole-5(2H)-carboxylate (9.41 g, 45 mmol) in 200 mL 1,2-dichloroethane was added N-iodosuccinimide (13.16 g, 58.5 mmol), and refluxed overnight. The solvent was removed by evaporation, and the residue was purified by silica gel column chromatography to give tert-butyl 3-iodo-4,6-dihydropyrrolo[3,4-c]pyrazole-5(2H)-carboxylate (4.66 g). Yield: 31%.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T12', 'WORKUP 455 463', 'purified'],\n",
       " ['T0', 'EXAMPLE_LABEL 121 122', '1'],\n",
       " ['T13', 'REACTION_STEP 375 383', 'refluxed']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'R']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_relations, test_missed = process_ann(f'{test_path}.ann', test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 'Χ', 'T6'), (14, 'O', 'X'), (16, 'Ο', 'T11')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R0', 'ARG1', 'T12', 'T7'),\n",
       " ('R1', 'ARG1', 'T14', 'T9'),\n",
       " ('R2', 'ARG1', 'T14', 'T5'),\n",
       " ('R3', 'ARG1', 'T14', 'T8'),\n",
       " ('R5', 'ARG1', 'T15', 'T3'),\n",
       " ('R6', 'ARGM', 'T15', 'T10'),\n",
       " ('R7', 'ARGM', 'T15', 'T2'),\n",
       " ('N0', 'NONE', 'T15', 'T7'),\n",
       " ('N1', 'NONE', 'T15', 'T5'),\n",
       " ('N2', 'NONE', 'T14', 'T4'),\n",
       " ('N3', 'NONE', 'T14', 'T11'),\n",
       " ('N4', 'NONE', 'T14', 'T7'),\n",
       " ('N5', 'NONE', 'T13', 'T5'),\n",
       " ('N6', 'NONE', 'T13', 'T9'),\n",
       " ('N7', 'NONE', 'T13', 'T8'),\n",
       " ('N8', 'NONE', 'T13', 'T4'),\n",
       " ('N9', 'NONE', 'T13', 'T7'),\n",
       " ('N10', 'NONE', 'T13', 'T3'),\n",
       " ('N11', 'NONE', 'T13', 'T10'),\n",
       " ('N12', 'NONE', 'T12', 'T5'),\n",
       " ('N13', 'NONE', 'T12', 'T9'),\n",
       " ('N14', 'NONE', 'T12', 'T8'),\n",
       " ('N15', 'NONE', 'T12', 'T3'),\n",
       " ('N16', 'NONE', 'T12', 'T10'),\n",
       " ('N17', 'NONE', 'T12', 'T2')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Intermediate ', 'O', 'X'), ('2', 'Χ', 'T6'), (': ', 'O', 'X')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000-R0',\n",
       " 'ARG1',\n",
       " 'The solvent was removed by evaporation and the residue was Λ purified [/E1] by Ο silica gel [/E2] column chromatography to give tertbutyl 3iodo46dihydropyrrolo34cpyrazole52Hcarboxylate 4.66 g .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_snippet = relation_input('0000', test_relations[0], trial_sentence)\n",
    "trial_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 0\n",
      "Finished in 0.110 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_re_files([test_path], '../raw_data/test', test=True)\n",
    "with io.open('../raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 0\n",
      "Finished in 0.053 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_re_files_subsampled([test_path], '../raw_data/test2')\n",
    "with io.open('../raw_data/test2.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output2 = sample.readlines()\n",
    "len(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 0\n",
      "Finished in 5.229 seconds\n",
      "Number of sample snippets: 1977\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = '../raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = '../data/sre_ner/sre_ner_sample'\n",
    "generate_re_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of sample snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = '../raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = '../raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = '../raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = '../raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: -2\n",
      "Finished in 186.369 seconds\n",
      "Number of train snippets: 45805\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/sre_ner/sre_ner_train'\n",
    "generate_re_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of train snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 0\n",
      "Finished in 35.089 seconds\n",
      "Number of dev sentences: 10673\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/sre_ner/sre_ner_dev'\n",
    "generate_re_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of dev sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 25\n",
      "Finished in 71.302 seconds\n",
      "Number of test sentences: 18488\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = '../data/sre_ner/sre_ner_test'\n",
    "generate_re_files(filepath_test, output_test, test=True)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8]]\n",
    "print(f'Number of test sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: -2\n",
      "Finished in 117.221 seconds\n",
      "Number of train snippets: 27771\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/sre_ner/sre_ner_train_subsampled'\n",
    "generate_re_files_subsampled(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of train snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive relations missed: 0\n",
      "Finished in 22.501 seconds\n",
      "Number of dev sentences: 6447\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/sre_ner/sre_ner_dev_subsampled'\n",
    "generate_re_files_subsampled(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of dev sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
