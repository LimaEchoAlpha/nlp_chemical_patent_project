{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from string import punctuation\n",
    "from time import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for identifying entities and relations \n",
    "       in corresponding .txt file.\n",
    "       Also adds negative relations within the span of +/- entities \n",
    "       from each trigger word.\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       relations = list of tuples for extracting relations from corresponding .txt file\n",
    "                   format: (relation ID, relation_type, entity ID #1, entity ID #2)\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = [x.strip().split('\\t') for x in f.readlines()]\n",
    "        \n",
    "    ann = [x for x in text if x[0][0] == 'T']\n",
    "    rel = [x for x in text if x[0][0] == 'R']\n",
    "    \n",
    "    # extract information for identifying entities\n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    # sort offsets and clean overlapping entries\n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    # extract information for identifying relations\n",
    "    relations = []\n",
    "    positives = []\n",
    "    \n",
    "    # add positive relations\n",
    "    for r in rel:\n",
    "        relation_id = r[0]\n",
    "        relation_type = r[1].split()[0]\n",
    "        entity1 = r[1].split()[1][5:]\n",
    "        entity2 = r[1].split()[2][5:]\n",
    "        \n",
    "        if entity1 in corrections.keys():\n",
    "            entity1 = corrections[entity1]\n",
    "        if entity2 in corrections.keys():\n",
    "            entity2 = corrections[entity2]\n",
    "        \n",
    "        relations.append((relation_id, relation_type, entity1, entity2))\n",
    "        positives.append((entity1, entity2))\n",
    "    \n",
    "    # negative relations\n",
    "    negatives = []\n",
    "    triggers = {x[2] for x in cleaned_offsets if (x[1] == 'WORKUP' or x[1] == 'REACTION_STEP')}\n",
    "    entity_order = [x[2] for x in cleaned_offsets if (x[2] != 'X' and x[1] != 'EXAMPLE_LABEL')]\n",
    "    trigger_indices = {i for i in range(len(entity_order)) if entity_order[i] in triggers}\n",
    "    \n",
    "    # find negative relations\n",
    "    for index in trigger_indices:\n",
    "        \n",
    "        # find indices in span of +/- 5 from trigger\n",
    "        find_span = [index - (i+1) for i in range(5)] + [index + (i+1) for i in range(5)]\n",
    "        real_span = [i for i in find_span if (i >= 0 and i < len(entity_order))]\n",
    "        span = [i for i in real_span if i not in trigger_indices]\n",
    "        \n",
    "        # make tuples of trigger words and entities from span indices\n",
    "        potential_pairs = [(entity_order[index], entity_order[i]) for i in span]\n",
    "        \n",
    "        # check if tuple is in positives\n",
    "        # only add to negatives if not in positives\n",
    "        for pair in potential_pairs:\n",
    "            if pair in positives:\n",
    "                continue\n",
    "            else:\n",
    "                negatives.append(pair)\n",
    "    \n",
    "    # make a list of negative relations \n",
    "    # in same format as positives to add to relations list\n",
    "    i = 0\n",
    "    for pair in negatives:\n",
    "        negative_id = f'N{i}'\n",
    "        relations.append((negative_id, 'NONE', pair[0], pair[1]))\n",
    "        i += 1\n",
    "    \n",
    "    return cleaned_offsets, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label, entity ID)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    entity_id = 'X'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label, entity_id)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        entity_id = offsets[i][2]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label, entity_id)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label, entity_id)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_input(snippet_id, rel_tup, chunks):\n",
    "    \"\"\"Helper function that creates one input snippet for BERT SRE:\n",
    "       Inserts entity markers and truncates snippet to include only\n",
    "       sentences containing the entities\n",
    "    \n",
    "       Inputs:\n",
    "       snippet_id = filename of snippet\n",
    "       rel_tup = tuple from relations list generated by process_ann()\n",
    "                 format: (relation ID, relation_type, entity ID #1, entity ID #2)\n",
    "       chunks = list of annotated chunks from ann_chunker()\n",
    "    \n",
    "       Output:\n",
    "       rel_input = input snippet ready for BERT SRE\n",
    "                   format: [snippet_id+relation_id]/t[relation_type]/t[cleaned snippet with ner markers]\"\"\"\n",
    "    \n",
    "    # unpack relation_tup\n",
    "    relation_id = rel_tup[0]\n",
    "    relation_type = rel_tup[1]\n",
    "    entity_list = [rel_tup[2], rel_tup[3]]\n",
    "    \n",
    "    new_id = snippet_id + '-' + relation_id\n",
    "    \n",
    "    # build cleaned snippet with ner markers\n",
    "    snippets = []\n",
    "    i = 1\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label, entity = tup\n",
    "        \n",
    "        # clean chunk: remove punctuation,\n",
    "        # word tokenize if not an entity\n",
    "        # split by whitespace if entity\n",
    "        processed_chunk = []\n",
    "            \n",
    "        if label == 'O':\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk.strip())\n",
    "            sentences = sent_tokenize(nopunct)\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        processed_chunk.append(x)\n",
    "                \n",
    "        else:\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk)\n",
    "            tokens = [x for x in nopunct.split(' ') if x]\n",
    "            for t in tokens:\n",
    "                processed_chunk.append(t)\n",
    "        \n",
    "        # add ner markers before and after entities in relation\n",
    "        if entity in entity_list:\n",
    "            snippets.append(f'[E{i}]')\n",
    "            snippets.extend(processed_chunk)\n",
    "            snippets.append(f'[/E{i}]')\n",
    "            i += 1\n",
    "        \n",
    "        else:\n",
    "            snippets.extend(processed_chunk)\n",
    "            \n",
    "    # keep only sentences containing the entities\n",
    "    # sentences are marked by periods\n",
    "    e1_index = snippets.index('[E1]')\n",
    "    e2_index = snippets.index('[/E2]')\n",
    "    \n",
    "    periods = [i for i in range(len(snippets)) if snippets[i] == '.']\n",
    "    periods_before = [i for i in periods if i < e1_index]\n",
    "    periods_after = [i for i in periods if i > e2_index]\n",
    "    \n",
    "    if periods_before:\n",
    "        start = max(periods_before) + 1\n",
    "    else:\n",
    "        start = 0\n",
    "    \n",
    "    if periods_after:\n",
    "        end = min(periods_after) + 1\n",
    "    else:\n",
    "        end = None\n",
    "    \n",
    "    truncated_snippet = snippets[start:end]\n",
    "\n",
    "    # join snippet chunks to one clean snippet\n",
    "    cleaned_snippet = ' '.join(truncated_snippet)\n",
    "    \n",
    "    return [new_id, relation_type, cleaned_snippet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_re_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file with snippets ready for BERT SRE (one snippet per line)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    snippets = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        snippet_id = file[-4:]\n",
    "        \n",
    "        cleaned_offsets, relations = process_ann(f'{file}.ann')\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        \n",
    "        for tup in relations:\n",
    "            line = relation_input(snippet_id, tup, chunks)\n",
    "            snippets.append(line)   \n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(snippets)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_re_files_subsampled(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file with snippets ready for BERT SRE (one snippet per line).\n",
    "       Subsampled 25% of negative relations randomly.\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    snippets = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        snippet_id = file[-4:]\n",
    "        \n",
    "        cleaned_offsets, relations = process_ann(f'{file}.ann')\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        \n",
    "        # add all positive relations\n",
    "        positive_relations = [tup for tup in relations if tup[0][0] == 'R']\n",
    "        \n",
    "        for tup in positive_relations:\n",
    "            line = relation_input(snippet_id, tup, chunks)\n",
    "            snippets.append(line)\n",
    "        \n",
    "        # keep only 25% of negative relations\n",
    "        negative_relations = [tup for tup in relations if tup[0][0] == 'N']\n",
    "        neg_count = len(negative_relations)\n",
    "        np.random.seed(424)\n",
    "        negative_keep = np.random.binomial(1, 0.25, neg_count)\n",
    "        \n",
    "        for i in range(neg_count):\n",
    "            if negative_keep[i] == 1:\n",
    "                line = relation_input(snippet_id, negative_relations[i], chunks)\n",
    "                snippets.append(line)      \n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(snippets)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../raw_data/sample_ee/0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 194\\n3-Isobutyl-5-methyl-1-(oxetan-2-ylmethyl)-6-[(2-oxoimidazolidin-1-yl)methyl]thieno[2,3-d]pyrimidine-2,4(1H,3H)-dione (racemate)\\n813 mg (1.84 mmol) of the compound from Example 243A were dissolved in 40 ml of dioxane, and 461 mg (2.76 mmol) of CDI were added. The mixture was stirred at RT for 16 h. The reaction solution was then concentrated on a rotary evaporator. The residue was dissolved in 15 ml of DMSO and this solution was purified by means of preparative HPLC (Method 14). Combination of the product fractions and freeze-drying gave 383 mg (42% of theory) of the title compound'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'OTHER_COMPOUND 417 421', 'DMSO'],\n",
       " ['T1', 'TIME 305 309', '16 h'],\n",
       " ['T2', 'REACTION_PRODUCT 585 599', 'title compound']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'R']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_relations = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'EXAMPLE_LABEL', 'T8'), (11, 'O', 'X'), (12, 'REACTION_PRODUCT', 'T6')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R0', 'ARGM', 'T11', 'T10'),\n",
       " ('R1', 'ARGM', 'T11', 'T3'),\n",
       " ('R2', 'ARG1', 'T11', 'T2')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_relations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Example ', 'O', 'X'), ('194', 'EXAMPLE_LABEL', 'T8'), ('\\n', 'O', 'X')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000-R0',\n",
       " 'ARGM',\n",
       " 'Combination of the product fractions and freezedrying [E1] gave [/E1] [E2] 383 mg [/E2] 42% of theory of the title compound']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_snippet = relation_input('0000', test_relations[0], trial_sentence)\n",
    "trial_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.075 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_re_files([test_path], '../raw_data/test')\n",
    "with io.open('../raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.043 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_re_files_subsampled([test_path], '../raw_data/test2')\n",
    "with io.open('../raw_data/test2.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output2 = sample.readlines()\n",
    "len(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5.829 seconds\n",
      "Number of sample snippets: 1977\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = '../raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = '../data/sre_em/sre_em_sample'\n",
    "generate_re_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of sample snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = '../raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = '../raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = '../raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = '../raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 185.286 seconds\n",
      "Number of train snippets: 45821\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/sre_em/sre_em_train'\n",
    "generate_re_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of train snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 34.662 seconds\n",
      "Number of dev sentences: 10673\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/sre_em/sre_em_dev'\n",
    "generate_re_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of dev sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 73.260 seconds\n",
      "Number of test sentences: 18515\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = '../data/sre_em/sre_em_test'\n",
    "generate_re_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8]]\n",
    "print(f'Number of test sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 94.293 seconds\n",
      "Number of train snippets: 21911\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/sre_em/sre_em_train_subsampled'\n",
    "generate_re_files_subsampled(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of train snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 17.590 seconds\n",
      "Number of dev sentences: 5095\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/sre_em/sre_em_dev_subsampled'\n",
    "generate_re_files_subsampled(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of dev sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
