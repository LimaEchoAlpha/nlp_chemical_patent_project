{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QJ_ExX95C8m"
   },
   "source": [
    "### SRE Models\n",
    "Implementations of models using the fixed length relation representation schemes described in Google's [Matching the Blanks](https://arxiv.org/pdf/1906.03158.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10782,
     "status": "ok",
     "timestamp": 1637371488548,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjimsJwqZyDGc9HCTSMziSaLt9DBHq1F3Odv5B9=s64",
      "userId": "16569730562812745412"
     },
     "user_tz": 300
    },
    "id": "CckxGheg4_bq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sre_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sre_models.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "def sre_cls_model(bert_model, max_length, train_layers=0):\n",
    "    \"\"\"\n",
    "    Implementation of a Single Relation Extraction (SRE) Model \n",
    "    with [CLS] fixed length relation representation\n",
    "    Reference: https://arxiv.org/pdf/1906.03158.pdf\n",
    "    \n",
    "    Variables:\n",
    "    bert_model = pre-trained BERT model to be used as bert_layer\n",
    "    max_length = number of tokens per snippet entry\n",
    "    train_layers = number of layers to be retrained (optional)\n",
    "    \n",
    "    Returns: \n",
    "    Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # input placeholders\n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='segment_ids')\n",
    "    \n",
    "    inputs = [in_id, in_mask, in_segment]\n",
    "    bert_inputs = [inputs[0], inputs[2]]\n",
    "    bert_layer = bert_model\n",
    "    \n",
    "    # optional: freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    if not train_layers == -1:\n",
    "        retrain_layers = []\n",
    "        for retrain_layer_number in range(train_layers):\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "    # end of freezing section\n",
    "    \n",
    "    # pick out representation for [CLS] token\n",
    "    bert_output = bert_layer(bert_inputs)\n",
    "    cls = bert_output[:, 0, :]\n",
    "    \n",
    "    # post transformer layers (3): \n",
    "    # dense with linear activation, drop out, and prediction    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(cls)\n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    predictions = tf.keras.layers.Dense(3, activation='softmax', name='sre')(dense)\n",
    "    \n",
    "    # build model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name='sre_cls')\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.Accuracy()\n",
    "    categorical_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer,\n",
    "                  metrics=[accuracy, categorical_accuracy, recall, precision])\n",
    "    \n",
    "    print()\n",
    "    print(\"=== SRE [CLS] Model ===\")\n",
    "    print('BERT layer output:', bert_output)\n",
    "    print('Prediction:', predictions)\n",
    "    print()\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sre_start_model(bert_model, max_length, train_layers=0):\n",
    "    \"\"\"\n",
    "    Implementation of a Single Relation Extraction (SRE) Model \n",
    "    with Entity Start State: obtains a fixed length relation representation\n",
    "    by concatenating the final hidden states corresponding to the \n",
    "    start tokens of each entity\n",
    "    Reference: https://arxiv.org/pdf/1906.03158.pdf\n",
    "    \n",
    "    Variables:\n",
    "    bert_model = pre-trained BERT model to be used as bert_layer\n",
    "    max_length = number of tokens per snippet entry\n",
    "    train_layers = number of layers to be retrained (optional)\n",
    "    \n",
    "    Returns: \n",
    "    Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # input placeholders\n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='segment_ids')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.bool, name='e1_mask')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.bool, name='e2_mask')\n",
    "    \n",
    "    inputs = [in_id, in_mask, in_segment, e1_mask, e2_mask]\n",
    "    bert_inputs = [inputs[0], inputs[2]]\n",
    "    bert_layer = bert_model\n",
    "    \n",
    "    # optional: freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    if not train_layers == -1:\n",
    "        retrain_layers = []\n",
    "        for retrain_layer_number in range(train_layers):\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "    # end of freezing section\n",
    "    \n",
    "    bert_output = bert_layer(bert_inputs)\n",
    "    \n",
    "    # apply masks to pick out start entity tokens\n",
    "    e1_start = tf.ragged.boolean_mask(bert_output, e1_mask, name='e1_mention')\n",
    "    e1_start = tf.squeeze(e1_start, axis=1)\n",
    "    e2_start = tf.ragged.boolean_mask(bert_output, e2_mask, name='e2_mention')\n",
    "    e2_start = tf.squeeze(e2_start, axis=1)\n",
    "    \n",
    "    # concatenate start entity representations\n",
    "    dense_input = tf.keras.layers.Concatenate()([e1_start, e2_start])\n",
    "    \n",
    "    # post transformer layers (3): \n",
    "    # dense with linear activation, drop out, and prediction \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(dense_input)\n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    predictions = tf.keras.layers.Dense(3, activation='softmax', name='sre')(dense)\n",
    "    \n",
    "    # build model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name='sre_start')\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.Accuracy()\n",
    "    categorical_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer,\n",
    "                  metrics=[accuracy, categorical_accuracy, recall, precision])\n",
    "    \n",
    "    print()\n",
    "    print(\"=== SRE Start Entity Model ===\")\n",
    "    print('BERT layer output:', bert_output)\n",
    "    print('Prediction:', predictions)\n",
    "    print()\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sre_pool_model(bert_model, max_length, train_layers=0):\n",
    "    \"\"\"\n",
    "    Implementation of a Single Relation Extraction (SRE) Model \n",
    "    with Mention Pooling: obtains a fixed length relation representation\n",
    "    by max pooling the final hidden layers corresponding to the two entities\n",
    "    and concatenating the two vectors\n",
    "    Reference: https://arxiv.org/pdf/1906.03158.pdf\n",
    "    \n",
    "    Variables:\n",
    "    bert_model = pre-trained BERT model to be used as bert_layer\n",
    "    max_length = number of tokens per snippet entry\n",
    "    train_layers = number of layers to be retrained (optional)\n",
    "    \n",
    "    Returns: \n",
    "    Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # input placeholders\n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name='segment_ids')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.bool, name='e1_mask')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.bool, name='e2_mask')\n",
    "    \n",
    "    inputs = [in_id, in_mask, in_segment, e1_mask, e2_mask]\n",
    "    bert_inputs = [inputs[0], inputs[2]]\n",
    "    bert_layer = bert_model\n",
    "    \n",
    "    # optional: freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    if not train_layers == -1:\n",
    "        retrain_layers = []\n",
    "        for retrain_layer_number in range(train_layers):\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "    # end of freezing section\n",
    "\n",
    "    bert_output = bert_layer(bert_inputs)\n",
    "    \n",
    "    # apply masks to pick out outputs for mention tokens    \n",
    "    e1_mention = tf.ragged.boolean_mask(bert_output, e1_mask, name='e1_mention')\n",
    "    e1_mention = e1_mention.to_tensor()\n",
    "    e2_mention = tf.ragged.boolean_mask(bert_output, e2_mask, name='e2_mention')\n",
    "    e2_mention = e2_mention.to_tensor()\n",
    "    \n",
    "    # max pool entity mentions\n",
    "    e1_max = tf.math.reduce_max(e1_mention, axis=1, name='e1_max')\n",
    "    e2_max = tf.math.reduce_max(e2_mention, axis=1, name='e2_max')\n",
    "    \n",
    "    # concatenate max pooled entity mentions\n",
    "    dense_input = tf.keras.layers.Concatenate()([e1_max, e2_max])\n",
    "    \n",
    "    # post transformer layers (3): \n",
    "    # dense with linear activation, drop out, and prediction\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(dense_input)\n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    predictions = tf.keras.layers.Dense(3, activation='softmax', name='sre')(dense)\n",
    "    \n",
    "    # build model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name='sre_pool')\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.Accuracy()\n",
    "    categorical_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer,\n",
    "                  metrics=[accuracy, categorical_accuracy, recall, precision])\n",
    "    \n",
    "    print()\n",
    "    print(\"=== SRE Max Pool Model ===\")\n",
    "    print('BERT layer output:', bert_output)\n",
    "    print('Prediction:', predictions)\n",
    "    print()\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbXeVp7ZkpXlI1z13KHxK1",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1ncM7IFEgM8vhjwuZRLUJBL3L9Bh-70pS",
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
