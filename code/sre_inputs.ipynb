{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRE Inputs\n",
    "Functions for generating inputs to SRE models. Includes standard inputs and inputs that use entity markers.\n",
    "\n",
    "Two types of entity markers:\n",
    "- Entity markers described in Google's [Matching the Blanks](https://arxiv.org/pdf/1906.03158.pdf) paper\n",
    "- NER markers inspired by a paper by [Han, et al.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9098945) about document-level relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sre_inputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sre_inputs.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "def generate_entity_start_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask \n",
    "    that picks out the start marker for each entity \n",
    "    given a list of snippet tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e1_mask[np.argwhere(np.array(snippetTokens) == start1)] = True\n",
    "\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask[np.argwhere(np.array(snippetTokens) == start2)] = True\n",
    "\n",
    "    return e1_mask, e2_mask\n",
    "\n",
    "\n",
    "def generate_entity_mention_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask\n",
    "    that picks out the tokens for each entity\n",
    "    between (but not including) the entity markers\n",
    "    \"\"\"\n",
    "    \n",
    "    em_markers = [start1, '[/E1]', start2, '[/E2]']\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    in_e1 = False\n",
    "    in_e2 = False\n",
    "    \n",
    "    for (i, t) in enumerate(snippetTokens):\n",
    "        if t in em_markers:\n",
    "            if t in [start1, '[/E1]']:\n",
    "                in_e1 = not in_e1\n",
    "            elif t in [start2, '[/E2]']:\n",
    "                in_e2 = not in_e2\n",
    "        else:\n",
    "            if in_e1 is True:\n",
    "                e1_mask[i] = True\n",
    "            elif in_e2 is True:\n",
    "                e2_mask[i] = True\n",
    "                \n",
    "    return e1_mask, e2_mask\n",
    "\n",
    "\n",
    "def generate_ner_mention_mask(snippetTokens, max_length, start1, start2):\n",
    "    \"\"\"\n",
    "    Helper function that generates a mask\n",
    "    that picks out the tokens for each entity\n",
    "    between the entity markers, including the ner marker\n",
    "    \"\"\"\n",
    "    \n",
    "    em_markers = [start1, '[/E1]', start2, '[/E2]']\n",
    "    \n",
    "    e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "    in_e1 = False\n",
    "    in_e2 = False\n",
    "    \n",
    "    for (i, t) in enumerate(snippetTokens):\n",
    "        if t in em_markers:\n",
    "            if t in [start1, '[/E1]']:\n",
    "                in_e1 = not in_e1\n",
    "            elif t in [start2, '[/E2]']:\n",
    "                in_e2 = not in_e2\n",
    "        else:\n",
    "            if in_e1 is True:\n",
    "                e1_mask[i] = True\n",
    "            elif in_e2 is True:\n",
    "                e2_mask[i] = True\n",
    "    \n",
    "    x1 = snippetTokens.index(start1)\n",
    "    e1_mask[x1] = True\n",
    "    \n",
    "    x2 = snippetTokens.index(start2)\n",
    "    e2_mask[x2] = True\n",
    "                \n",
    "    return e1_mask, e2_mask\n",
    "\n",
    "\n",
    "\n",
    "def generate_entity_inputs(full_path, tokenizer, marker_type, head_type, max_length=500):\n",
    "    \"\"\"\n",
    "    Reads preprocessed chemical patent data for relation extraction line by line and\n",
    "    constructs arrays for input into BERT models. Also keeps track of snippet lengths\n",
    "    for EDA and IDs of any discarded entries.\n",
    "    \n",
    "    Each snippet is capped at max_length: snippets that are shorter are padded, and\n",
    "    snippets that are longer are truncated. All snippets end with a [SEP] token, padded or not.\n",
    "    Truncated snippets that only contain one entry are discarded.\n",
    "    \n",
    "    Inputs:\n",
    "    full_path = full path of chemical patent data file\n",
    "    tokenizer = loaded tokenizer to be used\n",
    "    marker_type = denotes whether the file uses entity markers ('em')\n",
    "                  or ner markers ('ner')\n",
    "    head_type = denotes the type of fixed length representation\n",
    "                the inputs are intended for: \n",
    "                'cls' = no entity masks \n",
    "                'start' = entity masks to pick out start tokens for each entity\n",
    "                'pool' = entity masks to pick out entity tokens between markers\n",
    "                'ner' = entity masks to pick out entity tokens plus ner marker\n",
    "    max_length = max length for capping snippets\n",
    "    \n",
    "    Outputs:\n",
    "    all_lists = [bert_inputs, bert_labels, extras]\n",
    "    bert_inputs: array of arrays for input into BERT model\n",
    "                 includes tokenIDs, bert masks, sequence IDs, and entity masks\n",
    "    bert_labels: array of labels (need to be one hot encoded before use in model)\n",
    "    extras: list of lists with extra information \n",
    "            includes original labels, snippet lengths, and discarded entries    \n",
    "    \"\"\"\n",
    "    \n",
    "    # lists for BERT input\n",
    "    bertTokenIDs = []\n",
    "    bertMasks = []\n",
    "    bertSeqIDs = []\n",
    "    \n",
    "    # list for labels\n",
    "    origLabels = []\n",
    "    codedLabels = []\n",
    "\n",
    "    # lists for entity masks\n",
    "    entity1Masks = []\n",
    "    entity2Masks = []\n",
    "    \n",
    "    # lists for processing\n",
    "    snippetLengthList = []\n",
    "    discardedEntries = []\n",
    "    \n",
    "    # dictionary for converting labels to code\n",
    "    code = {'NONE': 0, 'ARG1': 1, 'ARGM': 2}\n",
    "\n",
    "    # determine which marker list to use\n",
    "    if marker_type == 'em':\n",
    "        markers = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "    elif marker_type == 'ner':\n",
    "        markers = ['Α', 'Β', 'Π', 'Σ', 'Ο', 'Τ', 'Θ', 'Ψ', 'Υ', 'Χ', 'Λ', 'Δ', '[/E1]', '[/E2]']\n",
    "        \n",
    "    \n",
    "    # open file and read lines of text\n",
    "    # each line is an entry\n",
    "    with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    for line in text:\n",
    "\n",
    "        parsed_line = line.strip().split('\\t')\n",
    "\n",
    "        snippet_id = parsed_line[0]\n",
    "        label = parsed_line[1]\n",
    "        snippet = parsed_line[2].split()\n",
    "\n",
    "        # tokenize snippet, except for entity markers\n",
    "        # identify start markers for each entity\n",
    "        snippetTokens = ['[CLS]']\n",
    "        start1 = ''\n",
    "        start2 = ''\n",
    "        i = 1\n",
    "\n",
    "        for word in snippet:\n",
    "            if word not in markers:\n",
    "                tokens = tokenizer.tokenize(word)\n",
    "                snippetTokens.extend(tokens)\n",
    "            else:\n",
    "                snippetTokens.append(word)\n",
    "                if i == 1:\n",
    "                    start1 = word\n",
    "                if i == 3:\n",
    "                    start2 = word\n",
    "                i += 1\n",
    "\n",
    "        # check that both entities will make it within max_length\n",
    "        # by finding the index for [/E2] and comparing it to (max_length - 1)\n",
    "        check = snippetTokens.index('[/E2]')\n",
    "\n",
    "        # discard if only one entity will make it\n",
    "        if check >= (max_length - 1):\n",
    "            discardedEntries.append(f'{snippet_id}-length')\n",
    "            continue\n",
    "\n",
    "        # create space for at least a final [SEP] token\n",
    "        if len(snippetTokens) >= max_length:\n",
    "            snippetTokens = snippetTokens[:(max_length - 1)]\n",
    "        \n",
    "        # figure out snippet length for padding or truncating\n",
    "        snippetLength = len(snippetTokens) + 1\n",
    "        snippetLengthList.append(snippetLength - 2)\n",
    "\n",
    "        # add [SEP] token and padding\n",
    "        snippetTokens += ['[SEP]'] + ['[PAD]'] * (max_length - snippetLength)\n",
    "        \n",
    "        # generate entity masks\n",
    "        if head_type == 'start':\n",
    "            e1_mask, e2_mask = generate_entity_start_mask(snippetTokens, max_length, start1, start2)\n",
    "            \n",
    "            if sum(e1_mask) != 1 or sum(e2_mask) != 1:\n",
    "                snippetLengthList.pop()\n",
    "                discardedEntries.append(f'{snippet_id}-mask')\n",
    "                continue\n",
    "            else:\n",
    "                entity1Masks.append(e1_mask)\n",
    "                entity2Masks.append(e2_mask)\n",
    "        \n",
    "        elif head_type == 'pool':\n",
    "            e1_mask, e2_mask = generate_entity_mention_mask(snippetTokens, max_length, start1, start2)\n",
    "            entity1Masks.append(e1_mask)\n",
    "            entity2Masks.append(e2_mask)\n",
    "        \n",
    "        elif head_type == 'ner':\n",
    "            e1_mask, e2_mask = generate_ner_mention_mask(snippetTokens, max_length, start1, start2)\n",
    "            entity1Masks.append(e1_mask)\n",
    "            entity2Masks.append(e2_mask)\n",
    "\n",
    "        # generate BERT input lists\n",
    "        bertTokenIDs.append(tokenizer.convert_tokens_to_ids(snippetTokens))\n",
    "        bertMasks.append(([1] * snippetLength) + ([0] * (max_length - snippetLength)))\n",
    "        bertSeqIDs.append([0] * (max_length))\n",
    "\n",
    "        # generate label lists\n",
    "        origLabels.append(label)\n",
    "        codedLabels.append(code[label])\n",
    "\n",
    "    # convert bert inputs to np arrays for modeling\n",
    "    bert_inputs = [np.array(bertTokenIDs), np.array(bertMasks), np.array(bertSeqIDs), \n",
    "                   np.array(entity1Masks), np.array(entity2Masks)]\n",
    "    \n",
    "    # convert labels to one hot encoded for modeling\n",
    "    codedLabels_array = np.array(codedLabels)\n",
    "    #bert_labels = tf.one_hot(codedLabels_array, depth=2)\n",
    "    \n",
    "    # collect everything\n",
    "    extras = [origLabels, snippetLengthList, discardedEntries]\n",
    "    all_lists = [bert_inputs, codedLabels_array, extras]\n",
    "    \n",
    "    return all_lists\n",
    "\n",
    "\n",
    "\n",
    "def generate_standard_inputs(full_path, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Reads preprocessed chemical patent data for relation extraction line by line and\n",
    "    constructs arrays for standard input (i.e., no entity markers) into BERT models. \n",
    "    Also keeps track of snippet lengths for EDA and IDs of any discarded entries.\n",
    "    \n",
    "    Each snippet is capped at max_length: snippets that are shorter are padded, and\n",
    "    snippets that are longer are truncated. All snippets end with a [SEP] token, padded or not.\n",
    "    Truncated snippets that only contain one entry are discarded.\n",
    "    \n",
    "    Inputs:\n",
    "    full_path = full path of chemical patent data file\n",
    "    tokenizer = loaded tokenizer to be used\n",
    "    max_length = max length for capping snippets\n",
    "    \n",
    "    Outputs:\n",
    "    all_lists = [bert_inputs, bert_labels, extras]\n",
    "    bert_inputs: list of numpy arrays for inputs into BERT model\n",
    "                 includes tokenIDs, bert masks, sequence IDs, and entity masks\n",
    "    bert_labels: array of labels (need to be one hot encoded before use in model)\n",
    "    extras: list of lists with extra information \n",
    "            includes original labels, snippet lengths, and discarded entries    \n",
    "    \"\"\"\n",
    "    \n",
    "    # lists for BERT input\n",
    "    bertTokenIDs = []\n",
    "    bertMasks = []\n",
    "    bertSeqIDs = []\n",
    "    \n",
    "    # list for labels\n",
    "    origLabels = []\n",
    "    codedLabels = []\n",
    "\n",
    "    # lists for entity masks\n",
    "    entity1Masks = []\n",
    "    entity2Masks = []\n",
    "    \n",
    "    # lists for processing\n",
    "    snippetLengthList = []\n",
    "    discardedEntries = []\n",
    "    \n",
    "    # dictionary for converting labels to code\n",
    "    code = {'NONE': 0, 'ARG1': 1, 'ARGM': 2}\n",
    "\n",
    "    # determine which marker list to use\n",
    "    markers = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "        \n",
    "    \n",
    "    # open file and read lines of text\n",
    "    # each line is an entry\n",
    "    with io.open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    for line in text:\n",
    "\n",
    "        parsed_line = line.strip().split('\\t')\n",
    "\n",
    "        snippet_id = parsed_line[0]\n",
    "        label = parsed_line[1]\n",
    "        snippet = parsed_line[2].split()\n",
    "\n",
    "        # tokenize snippet, remove entity markers\n",
    "        # collect masking information for each entity\n",
    "        snippetTokens = ['[CLS]']\n",
    "        entity1 = []\n",
    "        e1 = 0\n",
    "        entity2 = []\n",
    "        e2 = 0\n",
    "        i = 1        \n",
    "\n",
    "        for word in snippet:\n",
    "            if word not in markers:\n",
    "                tokens = tokenizer.tokenize(word)\n",
    "                snippetTokens.extend(tokens)\n",
    "                entity1.extend([e1]*len(tokens))\n",
    "                entity2.extend([e2]*len(tokens))\n",
    "                i += len(tokens)\n",
    "            else:\n",
    "                if word == '[E1]':\n",
    "                    e1 = 1\n",
    "                elif word == '[/E1]':\n",
    "                    e1 = 0\n",
    "                elif word == '[E2]':\n",
    "                    e2 = 1\n",
    "                elif word == '[/E2]':\n",
    "                    e2 = 0\n",
    "                    # check for whether both entities\n",
    "                    # will make it within max length\n",
    "                    check = i - 1\n",
    "\n",
    "        # discard if only one entity will make it\n",
    "        if check >= (max_length - 1):\n",
    "            discardedEntries.append(snippet_id)\n",
    "            continue\n",
    "\n",
    "        # create space for at least a final [SEP] token\n",
    "        if len(snippetTokens) >= max_length:\n",
    "            snippetTokens = snippetTokens[:(max_length - 1)]\n",
    "        \n",
    "        # figure out snippet length for padding or truncating\n",
    "        snippetLength = len(snippetTokens) + 1\n",
    "        snippetLengthList.append(snippetLength - 2)\n",
    "\n",
    "        # add [SEP] token and padding\n",
    "        snippetTokens += ['[SEP]'] + ['[PAD]'] * (max_length - snippetLength)\n",
    "\n",
    "        # generate BERT input lists\n",
    "        bertTokenIDs.append(tokenizer.convert_tokens_to_ids(snippetTokens))\n",
    "        bertMasks.append(([1] * snippetLength) + ([0] * (max_length - snippetLength)))\n",
    "        bertSeqIDs.append([0] * (max_length))\n",
    "\n",
    "        # generate label lists\n",
    "        origLabels.append(label)\n",
    "        codedLabels.append(code[label])\n",
    "        \n",
    "        # generate entity masks\n",
    "        e1_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "        e1_mask[np.argwhere(np.array(entity1) == 1)] = True\n",
    "\n",
    "        e2_mask = np.zeros(shape=(max_length,), dtype=bool)\n",
    "        e2_mask[np.argwhere(np.array(entity2) == 1)] = True\n",
    "        \n",
    "        entity1Masks.append(e1_mask)\n",
    "        entity2Masks.append(e2_mask)\n",
    "        \n",
    "        \n",
    "    # convert bert inputs to np arrays for modeling\n",
    "    bert_inputs = [np.array(bertTokenIDs), np.array(bertMasks), np.array(bertSeqIDs), \n",
    "                   np.array(entity1Masks), np.array(entity2Masks)]\n",
    "    \n",
    "    # convert labels to one hot encoded for modeling\n",
    "    codedLabels_array = np.array(codedLabels)\n",
    "    #bert_labels = tf.one_hot(codedLabels_array, depth=2)\n",
    "    \n",
    "    # collect everything\n",
    "    extras = [origLabels, snippetLengthList, discardedEntries]\n",
    "    all_lists = [bert_inputs, codedLabels_array, extras]\n",
    "    \n",
    "    return all_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
