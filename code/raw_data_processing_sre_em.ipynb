{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(ann_file):\n",
    "    \"\"\"Helper function that reads a .ann file,\n",
    "       strips out newline characters, splits the tab-delimited entries,\n",
    "       and extracts information for identifying entities and relations \n",
    "       in corresponding .txt file\n",
    "       \n",
    "       Input:\n",
    "       ann_file = tab-delimited brat annotation file with the following format\n",
    "                  NER: [entity_ID]\\t[label start_offset end_offset]\\t[entity]\n",
    "                  RE:  [relation_ID]\\t[relation_type argument1 argument2]\n",
    "       \n",
    "       Outputs:\n",
    "       cleaned_offsets = list of tuples for labeling corresponding .txt file\n",
    "                         format: (offset, label, entity ID)\n",
    "       relations = list of tuples for extracting relations from corresponding .txt file\n",
    "                   format: (relation ID, relation_type, entity ID #1, entity ID #2)\"\"\"\n",
    "    \n",
    "    with io.open(ann_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = [x.strip().split('\\t') for x in f.readlines()]\n",
    "        \n",
    "    ann = [x for x in text if x[0][0] == 'T']\n",
    "    rel = [x for x in text if x[0][0] == 'R']\n",
    "    \n",
    "    # extract information for identifying entities\n",
    "    offsets = []\n",
    "    \n",
    "    for x in ann:\n",
    "        entity_id = x[0]\n",
    "        start = int(x[1].split()[1])\n",
    "        end = int(x[1].split()[2])\n",
    "        label = x[1].split()[0]\n",
    "        \n",
    "        offsets.append((start, 'S', label, entity_id))\n",
    "        offsets.append((end, 'E', label, entity_id))\n",
    "    \n",
    "    # sort offsets and clean overlapping entries\n",
    "    sorted_offsets = sorted(offsets, key=lambda x:x[0])\n",
    "    \n",
    "    cleaned_offsets = []\n",
    "    corrections = {}\n",
    "    \n",
    "    hold = None\n",
    "    indicator = None\n",
    "    \n",
    "    for tup in sorted_offsets:\n",
    "        \n",
    "        if indicator == 'S':\n",
    "            if tup[1] == 'E':\n",
    "                cleaned_offsets.append(hold)\n",
    "                hold = (tup[0], 'O', 'X')\n",
    "                indicator = tup[1]\n",
    "            elif tup[1] == 'S':\n",
    "                corrections.update({tup[3]:hold[2]})\n",
    "                indicator = '*'\n",
    "        \n",
    "        elif indicator == 'E':\n",
    "            cleaned_offsets.append(hold)\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "        \n",
    "        elif indicator == '*':\n",
    "            indicator = 'S'\n",
    "\n",
    "        else:\n",
    "            hold = (tup[0], tup[2], tup[3])\n",
    "            indicator = tup[1]\n",
    "            \n",
    "    cleaned_offsets.append(hold)\n",
    "    \n",
    "    # extract information for identifying relations\n",
    "    relations = []\n",
    "    \n",
    "    for r in rel:\n",
    "        relation_id = r[0]\n",
    "        relation_type = r[1].split()[0]\n",
    "        entity1 = r[1].split()[1][5:]\n",
    "        entity2 = r[1].split()[2][5:]\n",
    "        \n",
    "        if entity1 in corrections.keys():\n",
    "            entity1 = corrections[entity1]\n",
    "        if entity2 in corrections.keys():\n",
    "            entity2 = corrections[entity2]\n",
    "        \n",
    "        relations.append((relation_id, relation_type, entity1, entity2))\n",
    "    \n",
    "    return cleaned_offsets, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_chunker(txt_file, offsets):\n",
    "    \"\"\"Helper function that reads in a .txt file as one string,\n",
    "       divides it based on the cleaned offsets from its .ann file\n",
    "       and labels chunks with NER tags\n",
    "       \n",
    "       Inputs:\n",
    "       txt_file = file that contains all the patent text\n",
    "                  considered as one sentence in this task\n",
    "       offsets = list of tuples for labeling corresponding .txt file\n",
    "                 format: (offset, label, entity ID)\n",
    "       \n",
    "       Output:\n",
    "       ann_chunks = list of annotated chunks based on .ann file offsets\n",
    "                    format: (chunk, label, entity ID)\"\"\"\n",
    "    \n",
    "    with io.open(txt_file, 'r', encoding='utf-8', errors='ignore') as text:\n",
    "        full_text = text.read()\n",
    "    \n",
    "    start = 0\n",
    "    end = offsets[0][0]\n",
    "    label = 'O'\n",
    "    entity_id = 'X'\n",
    "    \n",
    "    ann_chunks = [(full_text[:end], label, entity_id)]\n",
    "    \n",
    "    for i in range(len(offsets)):\n",
    "        start = offsets[i][0]\n",
    "        label = offsets[i][1]\n",
    "        entity_id = offsets[i][2]\n",
    "        \n",
    "        if i < len(offsets) - 1:\n",
    "            end = offsets[i+1][0]\n",
    "            term = [(full_text[start:end], label, entity_id)]\n",
    "            if term[0]:\n",
    "                ann_chunks.extend(term)\n",
    "        \n",
    "        else:\n",
    "            term = [(full_text[start:], label, entity_id)]  \n",
    "            ann_chunks.extend(term)\n",
    "    \n",
    "    return ann_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_input(snippet_id, rel_tup, chunks):\n",
    "    \"\"\"Helper function that creates one input snippet for BERT SRE\n",
    "    \n",
    "    Inputs:\n",
    "    snippet_id = filename of snippet\n",
    "    rel_tup = tuple from relations list generated by process_ann()\n",
    "          format: (relation ID, relation_type, entity ID #1, entity ID #2)\n",
    "    chunks = list of annotated chunks from ann_chunker()\n",
    "    \n",
    "    Output:\n",
    "    rel_input = input snippet ready for BERT SRE\n",
    "                format: [snippet_id+relation_id]/t[relation_type]/t[cleaned snippet with ner markers]\"\"\"\n",
    "    \n",
    "    # unpack relation_tup\n",
    "    relation_id = rel_tup[0]\n",
    "    relation_type = rel_tup[1]\n",
    "    entity_list = [rel_tup[2], rel_tup[3]]\n",
    "    \n",
    "    new_id = snippet_id + '-' + relation_id\n",
    "    \n",
    "    # build cleaned snippet with ner markers\n",
    "    snippets = []\n",
    "    i = 1\n",
    "    \n",
    "    for tup in chunks:\n",
    "        chunk, label, entity = tup\n",
    "        \n",
    "        # clean chunk: remove punctuation,\n",
    "        # word tokenize if not an entity\n",
    "        # split by whitespace if entity\n",
    "        processed_chunk = []\n",
    "            \n",
    "        if label == 'O':\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk.strip())\n",
    "            sentences = sent_tokenize(nopunct)\n",
    "            if sentences:\n",
    "                for s in sentences:\n",
    "                    for x in word_tokenize(s):\n",
    "                        processed_chunk.append(x)\n",
    "                \n",
    "        else:\n",
    "            nopunct = re.sub(r'[,/()\":\\-\\[\\]\\']', '', chunk)\n",
    "            tokens = [x for x in nopunct.split(' ') if x]\n",
    "            for t in tokens:\n",
    "                processed_chunk.append(t)\n",
    "        \n",
    "        # add ner markers before and after entities in relation\n",
    "        if entity in entity_list:\n",
    "            snippets.append(f'[E{i}]')\n",
    "            snippets.extend(processed_chunk)\n",
    "            snippets.append(f'[/E{i}]')\n",
    "            i += 1\n",
    "        \n",
    "        else:\n",
    "            snippets.extend(processed_chunk)\n",
    "\n",
    "    # join snippet chunks to one clean snippet\n",
    "    cleaned_snippet = ' '.join(snippets)\n",
    "    \n",
    "    return [new_id, relation_type, cleaned_snippet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_re_files(filepaths, output_path):\n",
    "    \"\"\"Helper function that reads .txt and corresponding .ann files from a path\n",
    "       and generates csv file with snippets ready for BERT SRE (one snippet per line)\n",
    "       \n",
    "       Inputs:\n",
    "       filepaths = filepaths (folder + filename, but no extension) for .txt and .ann files\n",
    "       output_path = filepath (folder + filename, but no extension) for output file\"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    snippets = []\n",
    "    \n",
    "    for file in filepaths:\n",
    "        \n",
    "        snippet_id = file[-4:]\n",
    "        \n",
    "        cleaned_offsets, relations = process_ann(f'{file}.ann')\n",
    "        chunks = ann_chunker(f'{file}.txt', cleaned_offsets)\n",
    "        \n",
    "        for tup in relations:\n",
    "            line = relation_input(snippet_id, tup, chunks)\n",
    "            snippets.append(line)   \n",
    "    \n",
    "    with open(f'{output_path}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerows(snippets)\n",
    "    \n",
    "    end = time() - start\n",
    "    print(f'Finished in {end:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../raw_data/sample_ee/0015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[EXAMPLE 12]\\nSynthesis of compound I-044\\nStep 1 Synthesis of compound 44a\\n(R)-Isopropylidene glycerol (3.71 mL, 30.0 mmol) was dissolved in tetrahydrofuran (50.0 mL). The solution was cooled to 0째C. Triphenylphosphine (8.66 g, 33.0 mmol), N-hydroxyphthalimide (5.38 g, 33.0 mmol), and a 2.7 mol/L solution of dimethyl azodicarboxylate in toluene (12.22 mL, 33.0 mmol) were added to the solution. The mixture was stirred at room temperature for 30 minutes. The reaction mixture was concentrated under reduced pressure. Methanol was added to the residue. The mixture was stirred. The obtained solid was collected by filtration, washed by methanol, and dried to afford the compound 44a (1.78 g, yield 21%).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.txt', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    full_text = text.read()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'TEMPERATURE 423 439', 'room temperature'],\n",
       " ['T1', 'YIELD_OTHER 684 690', '1.78 g'],\n",
       " ['T2', 'YIELD_PERCENT 698 701', '21%']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.open(f'{test_path}.ann', 'r', encoding='utf-8', errors='ignore') as text:\n",
    "    ann = [x.strip().split('\\t') for x in text.readlines()] #if x.strip().split('\\t')[0][0] == 'R']\n",
    "ann[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_offsets, test_relations = process_ann(f'{test_path}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'EXAMPLE_LABEL', 'T13'), (11, 'O', 'X'), (35, 'OTHER_COMPOUND', 'T3')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_offsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R0', 'ARG1', 'T18', 'T9'),\n",
       " ('R1', 'ARGM', 'T18', 'T1'),\n",
       " ('R2', 'ARGM', 'T18', 'T2')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_relations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sentence = ann_chunker(f'{test_path}.txt', test_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[EXAMPLE ', 'O', 'X'),\n",
       " ('12', 'EXAMPLE_LABEL', 'T13'),\n",
       " (']\\nSynthesis of compound ', 'O', 'X')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000-R0',\n",
       " 'ARG1',\n",
       " 'EXAMPLE 12 Synthesis of compound I044 Step 1 Synthesis of compound 44a RIsopropylidene glycerol 3.71 mL 30.0 mmol was dissolved in tetrahydrofuran 50.0 mL . The solution was cooled to 0째C . Triphenylphosphine 8.66 g 33.0 mmol Nhydroxyphthalimide 5.38 g 33.0 mmol and a 2.7 molL solution of dimethyl azodicarboxylate in toluene 12.22 mL 33.0 mmol were added to the solution . The mixture was stirred at room temperature for 30 minutes . The reaction mixture was concentrated under reduced pressure . Methanol was added to the residue . The mixture was stirred . The obtained solid was collected by filtration washed by methanol and dried to [E1] afford [/E1] the compound [E2] 44a [/E2] 1.78 g yield 21% .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_snippet = relation_input('0000', test_relations[0], trial_sentence)\n",
    "trial_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.070 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0015-R0\\tARG1\\tEXAMPLE 12 Synthesis of compound I044 Step 1 Synthesis of compound 44a RIsopropylidene glycerol 3.71 mL 30.0 mmol was dissolved in tetrahydrofuran 50.0 mL . The solution was cooled to 0째C . Triphenylphosphine 8.66 g 33.0 mmol Nhydroxyphthalimide 5.38 g 33.0 mmol and a 2.7 molL solution of dimethyl azodicarboxylate in toluene 12.22 mL 33.0 mmol were added to the solution . The mixture was stirred at room temperature for 30 minutes . The reaction mixture was concentrated under reduced pressure . Methanol was added to the residue . The mixture was stirred . The obtained solid was collected by filtration washed by methanol and dried to [E1] afford [/E1] the compound [E2] 44a [/E2] 1.78 g yield 21% .\\n',\n",
       " '0015-R1\\tARGM\\tEXAMPLE 12 Synthesis of compound I044 Step 1 Synthesis of compound 44a RIsopropylidene glycerol 3.71 mL 30.0 mmol was dissolved in tetrahydrofuran 50.0 mL . The solution was cooled to 0째C . Triphenylphosphine 8.66 g 33.0 mmol Nhydroxyphthalimide 5.38 g 33.0 mmol and a 2.7 molL solution of dimethyl azodicarboxylate in toluene 12.22 mL 33.0 mmol were added to the solution . The mixture was stirred at room temperature for 30 minutes . The reaction mixture was concentrated under reduced pressure . Methanol was added to the residue . The mixture was stirred . The obtained solid was collected by filtration washed by methanol and dried to [E1] afford [/E1] the compound 44a [E2] 1.78 g [/E2] yield 21% .\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_re_files([test_path], '../raw_data/test')\n",
    "with io.open('../raw_data/test.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "output[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1.938 seconds\n",
      "Number of sample snippets: 658\n"
     ]
    }
   ],
   "source": [
    "# generate sample set\n",
    "path_sample = '../raw_data/sample_ee'\n",
    "filenames_sample = list({x[:4] for x in os.listdir(path_sample) if x[0] != '.'})\n",
    "filepath_sample = [f'{path_sample}/{x}' for x in filenames_sample]\n",
    "\n",
    "output_sample = '../data/sre_em/sre_em_sample'\n",
    "generate_re_files(filepath_sample, output_sample)\n",
    "\n",
    "with io.open(f'{output_sample}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of sample snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 900\n",
      "Number of dev files: 225\n",
      "Number of test files: 9999\n",
      "Number of test .ann files: 375\n"
     ]
    }
   ],
   "source": [
    "# generate filename list for train, dev, and test sets\n",
    "path_train = '../raw_data/EE/ee_train'\n",
    "filenames_train = list({x[:4] for x in os.listdir(path_train) if x[0] != '.'})\n",
    "print(f'Number of train files: {len(filenames_train)}')\n",
    "\n",
    "path_dev = '../raw_data/EE/ee_dev'\n",
    "filenames_dev = list({x[:4] for x in os.listdir(path_dev) if x[0] != '.'})\n",
    "print(f'Number of dev files: {len(filenames_dev)}')\n",
    "\n",
    "path_test = '../raw_data/EE/ee_test'\n",
    "filenames_test = list({x[:4] for x in os.listdir(path_test) if x[0] != '.'})\n",
    "print(f'Number of test files: {len(filenames_test)}')\n",
    "\n",
    "path_test_ann = '../raw_data/EE/ee_test_ann'\n",
    "filenames_test_ann = list({x[:4] for x in os.listdir(path_test_ann) if x[0] != '.'})\n",
    "print(f'Number of test .ann files: {len(filenames_test_ann)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many test .txt files match the .ann files\n",
    "intersect = list(set(filenames_test) & set(filenames_test_ann))\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 54.522 seconds\n",
      "Number of train snippets: 14310\n"
     ]
    }
   ],
   "source": [
    "# generate train set\n",
    "filepath_train = [f'{path_train}/{x}' for x in filenames_train]\n",
    "\n",
    "output_train = '../data/sre_em/sre_em_train'\n",
    "generate_re_files(filepath_train, output_train)\n",
    "\n",
    "with io.open(f'{output_train}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of train snippets: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 11.255 seconds\n",
      "Number of dev sentences: 3332\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "filepath_dev = [f'{path_dev}/{x}' for x in filenames_dev]\n",
    "\n",
    "output_dev = '../data/sre_em/sre_em_dev'\n",
    "generate_re_files(filepath_dev, output_dev)\n",
    "\n",
    "with io.open(f'{output_dev}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output]\n",
    "print(f'Number of dev sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 21.594 seconds\n",
      "Number of test sentences: 5803\n"
     ]
    }
   ],
   "source": [
    "# generate test set\n",
    "filepath_test = [f'{path_test}/{x}' for x in intersect]\n",
    "\n",
    "output_test = '../data/sre_em/sre_em_test'\n",
    "generate_re_files(filepath_test, output_test)\n",
    "\n",
    "with io.open(f'{output_test}.csv', 'r', encoding='utf-8', errors='ignore') as sample:\n",
    "    output = sample.readlines()\n",
    "check = [1 for x in output if x[:8]]\n",
    "print(f'Number of test sentences: {len(check)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
